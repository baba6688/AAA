#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
L3é”™è¯¯æ—¥å¿—è®°å½•å™¨æ¨¡å—

æœ¬æ¨¡å—æä¾›äº†ä¼ä¸šçº§çš„é”™è¯¯æ—¥å¿—è®°å½•å’Œå¤„ç†åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. å¼‚å¸¸æ•è·å’Œåˆ†ç±»è®°å½•ï¼ˆç³»ç»Ÿå¼‚å¸¸ã€ä¸šåŠ¡å¼‚å¸¸ã€ç½‘ç»œå¼‚å¸¸ï¼‰
2. é”™è¯¯å †æ ˆè·Ÿè¸ªå’Œä¸Šä¸‹æ–‡ä¿¡æ¯
3. é”™è¯¯ç»Ÿè®¡å’Œåˆ†æï¼ˆé”™è¯¯é¢‘ç‡ã€é”™è¯¯ç±»å‹ã€å½±å“èŒƒå›´ï¼‰
4. é”™è¯¯å‘Šè­¦å’Œé€šçŸ¥æœºåˆ¶
5. é”™è¯¯æ¢å¤å’Œé‡è¯•æ—¥å¿—
6. é”™è¯¯è§£å†³å’Œå…³é—­æµç¨‹
7. å¼‚æ­¥é”™è¯¯æ—¥å¿—å¤„ç†
8. å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•

"""

import asyncio
import logging
import json
import sqlite3
import threading
import time
import traceback
import uuid
import smtplib
import requests
from datetime import datetime, timedelta

# å¯é€‰å¯¼å…¥
try:
    import websocket
    WEBSOCKET_AVAILABLE = True
except ImportError:
    WEBSOCKET_AVAILABLE = False
from typing import Dict, List, Optional, Any, Callable, Union, Tuple
from dataclasses import dataclass, asdict
from enum import Enum, auto
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import wraps
from collections import defaultdict, deque
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import os
import sys
import psutil
import gc
import weakref
from pathlib import Path


class ErrorType(Enum):
    """é”™è¯¯ç±»å‹æšä¸¾"""
    SYSTEM = "system"          # ç³»ç»Ÿå¼‚å¸¸
    BUSINESS = "business"      # ä¸šåŠ¡å¼‚å¸¸
    NETWORK = "network"        # ç½‘ç»œå¼‚å¸¸
    DATABASE = "database"      # æ•°æ®åº“å¼‚å¸¸
    SECURITY = "security"      # å®‰å…¨å¼‚å¸¸
    PERFORMANCE = "performance" # æ€§èƒ½å¼‚å¸¸
    CONFIGURATION = "configuration" # é…ç½®å¼‚å¸¸
    UNKNOWN = "unknown"        # æœªçŸ¥å¼‚å¸¸


class ErrorSeverity(Enum):
    """é”™è¯¯ä¸¥é‡çº§åˆ«æšä¸¾"""
    CRITICAL = "critical"      # ä¸¥é‡
    HIGH = "high"              # é«˜
    MEDIUM = "medium"          # ä¸­
    LOW = "low"                # ä½
    INFO = "info"              # ä¿¡æ¯


class ErrorStatus(Enum):
    """é”™è¯¯çŠ¶æ€æšä¸¾"""
    OPEN = "open"              # æ‰“å¼€
    IN_PROGRESS = "in_progress" # å¤„ç†ä¸­
    RESOLVED = "resolved"      # å·²è§£å†³
    CLOSED = "closed"          # å·²å…³é—­
    IGNORED = "ignored"        # å·²å¿½ç•¥
    RECURRING = "recurring"    # é‡å¤å‘ç”Ÿ


class AlertChannel(Enum):
    """å‘Šè­¦æ¸ é“æšä¸¾"""
    EMAIL = "email"            # é‚®ä»¶
    WEBHOOK = "webhook"        # Webhook
    SMS = "sms"               # çŸ­ä¿¡
    SLACK = "slack"           # Slack
    DINGTALK = "dingtalk"     # é’‰é’‰
    WECHAT = "wechat"         # å¾®ä¿¡
    LOG = "log"               # æ—¥å¿—
    DATABASE = "database"     # æ•°æ®åº“


class RecoveryStrategy(Enum):
    """æ¢å¤ç­–ç•¥æšä¸¾"""
    RETRY = "retry"           # é‡è¯•
    FALLBACK = "fallback"     # é™çº§
    CIRCUIT_BREAKER = "circuit_breaker"  # ç†”æ–­å™¨
    BULKHEAD = "bulkhead"     # éš”æ¿æ¨¡å¼
    TIMEOUT = "timeout"       # è¶…æ—¶
    IGNORE = "ignore"         # å¿½ç•¥


@dataclass
class ErrorContext:
    """é”™è¯¯ä¸Šä¸‹æ–‡ä¿¡æ¯ç±»"""
    error_id: str
    timestamp: datetime
    error_type: ErrorType
    severity: ErrorSeverity
    message: str
    exception_type: str
    stack_trace: str
    module_name: str
    function_name: str
    line_number: int
    user_id: Optional[str] = None
    session_id: Optional[str] = None
    request_id: Optional[str] = None
    service_name: Optional[str] = None
    host_name: Optional[str] = None
    process_id: Optional[int] = None
    thread_id: Optional[int] = None
    memory_usage: Optional[float] = None
    cpu_usage: Optional[float] = None
    additional_data: Optional[Dict[str, Any]] = None
    custom_fields: Optional[Dict[str, Any]] = None

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        data = asdict(self)
        data['timestamp'] = self.timestamp.isoformat()
        data['error_type'] = self.error_type.value
        data['severity'] = self.severity.value
        return data

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'ErrorContext':
        """ä»å­—å…¸åˆ›å»ºå®ä¾‹"""
        data['timestamp'] = datetime.fromisoformat(data['timestamp'])
        data['error_type'] = ErrorType(data['error_type'])
        data['severity'] = ErrorSeverity(data['severity'])
        return cls(**data)


@dataclass
class ErrorStatistics:
    """é”™è¯¯ç»Ÿè®¡åˆ†æç±»"""
    total_errors: int = 0
    errors_by_type: Dict[ErrorType, int] = None
    errors_by_severity: Dict[ErrorSeverity, int] = None
    errors_by_module: Dict[str, int] = None
    errors_by_hour: Dict[int, int] = None
    recurring_errors: int = 0
    resolved_errors: int = 0
    avg_resolution_time: float = 0.0
    peak_error_time: Optional[datetime] = None
    error_frequency: Dict[str, int] = None
    affected_users: int = 0
    system_impact_score: float = 0.0

    def __post_init__(self):
        if self.errors_by_type is None:
            self.errors_by_type = {t: 0 for t in ErrorType}
        if self.errors_by_severity is None:
            self.errors_by_severity = {s: 0 for s in ErrorSeverity}
        if self.errors_by_module is None:
            self.errors_by_module = {}
        if self.errors_by_hour is None:
            self.errors_by_hour = {}
        if self.error_frequency is None:
            self.error_frequency = {}


class AlertManager:
    """é”™è¯¯å‘Šè­¦ç®¡ç†å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.alert_rules = config.get('alert_rules', {})
        self.notification_channels = config.get('notification_channels', {})
        self.alert_history = deque(maxlen=1000)
        self.alert_suppression = defaultdict(float)
        self._lock = threading.RLock()
    
    def should_alert(self, error_context: ErrorContext) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦å‘é€å‘Šè­¦"""
        with self._lock:
            # æ£€æŸ¥å‘Šè­¦æŠ‘åˆ¶
            suppression_key = f"{error_context.error_type.value}_{error_context.severity.value}"
            if suppression_key in self.alert_suppression:
                if time.time() - self.alert_suppression[suppression_key] < 300:  # 5åˆ†é’ŸæŠ‘åˆ¶
                    return False
            
            # æ£€æŸ¥å‘Šè­¦è§„åˆ™
            rule_key = f"{error_context.error_type.value}_{error_context.severity.value}"
            if rule_key in self.alert_rules:
                rule = self.alert_rules[rule_key]
                return rule.get('enabled', True)
            
            return error_context.severity in [ErrorSeverity.CRITICAL, ErrorSeverity.HIGH]
    
    def send_alert(self, error_context: ErrorContext, channels: List[AlertChannel] = None) -> bool:
        """å‘é€å‘Šè­¦é€šçŸ¥"""
        if channels is None:
            channels = [AlertChannel.LOG]
        
        success_count = 0
        with self._lock:
            for channel in channels:
                try:
                    if self._send_alert_by_channel(channel, error_context):
                        success_count += 1
                except Exception as e:
                    logging.error(f"å‘Šè­¦å‘é€å¤±è´¥ [{channel.value}]: {str(e)}")
            
            # è®°å½•å‘Šè­¦å†å²
            self.alert_history.append({
                'timestamp': time.time(),
                'error_id': error_context.error_id,
                'channels': [c.value for c in channels],
                'success_count': success_count,
                'total_count': len(channels)
            })
            
            # è®¾ç½®å‘Šè­¦æŠ‘åˆ¶
            suppression_key = f"{error_context.error_type.value}_{error_context.severity.value}"
            self.alert_suppression[suppression_key] = time.time()
        
        return success_count > 0
    
    def _send_alert_by_channel(self, channel: AlertChannel, error_context: ErrorContext) -> bool:
        """é€šè¿‡æŒ‡å®šæ¸ é“å‘é€å‘Šè­¦"""
        try:
            if channel == AlertChannel.EMAIL:
                return self._send_email_alert(error_context)
            elif channel == AlertChannel.WEBHOOK:
                return self._send_webhook_alert(error_context)
            elif channel == AlertChannel.SLACK:
                return self._send_slack_alert(error_context)
            elif channel == AlertChannel.DINGTALK:
                return self._send_dingtalk_alert(error_context)
            elif channel == AlertChannel.WECHAT:
                return self._send_wechat_alert(error_context)
            elif channel == AlertChannel.LOG:
                return self._send_log_alert(error_context)
            else:
                logging.warning(f"ä¸æ”¯æŒçš„å‘Šè­¦æ¸ é“: {channel.value}")
                return False
        except Exception as e:
            logging.error(f"å‘Šè­¦æ¸ é“ [{channel.value}] å‘é€å¤±è´¥: {str(e)}")
            return False
    
    def _send_email_alert(self, error_context: ErrorContext) -> bool:
        """å‘é€é‚®ä»¶å‘Šè­¦"""
        email_config = self.notification_channels.get('email', {})
        if not email_config:
            return False
        
        try:
            msg = MIMEMultipart()
            msg['From'] = email_config['from']
            msg['To'] = ', '.join(email_config['to'])
            msg['Subject'] = f"[{error_context.severity.value.upper()}] {error_context.error_type.value} é”™è¯¯å‘Šè­¦"
            
            body = f"""
é”™è¯¯è¯¦æƒ…:
- é”™è¯¯ID: {error_context.error_id}
- é”™è¯¯ç±»å‹: {error_context.error_type.value}
- ä¸¥é‡çº§åˆ«: {error_context.severity.value}
- é”™è¯¯æ¶ˆæ¯: {error_context.message}
- æ¨¡å—: {error_context.module_name}
- å‡½æ•°: {error_context.function_name}
- è¡Œå·: {error_context.line_number}
- æ—¶é—´: {error_context.timestamp}
- ä¸»æœº: {error_context.host_name}
- è¿›ç¨‹ID: {error_context.process_id}

å †æ ˆè·Ÿè¸ª:
{error_context.stack_trace}
            """
            
            msg.attach(MIMEText(body, 'plain', 'utf-8'))
            
            server = smtplib.SMTP(email_config['smtp_server'], email_config['smtp_port'])
            if email_config.get('use_tls'):
                server.starttls()
            if email_config.get('username') and email_config.get('password'):
                server.login(email_config['username'], email_config['password'])
            
            server.send_message(msg)
            server.quit()
            return True
        except Exception:
            return False
    
    def _send_webhook_alert(self, error_context: ErrorContext) -> bool:
        """å‘é€Webhookå‘Šè­¦"""
        webhook_config = self.notification_channels.get('webhook', {})
        if not webhook_config:
            return False
        
        try:
            payload = {
                'error_id': error_context.error_id,
                'error_type': error_context.error_type.value,
                'severity': error_context.severity.value,
                'message': error_context.message,
                'timestamp': error_context.timestamp.isoformat(),
                'module': error_context.module_name,
                'function': error_context.function_name,
                'stack_trace': error_context.stack_trace
            }
            
            response = requests.post(
                webhook_config['url'],
                json=payload,
                headers=webhook_config.get('headers', {}),
                timeout=10
            )
            return response.status_code == 200
        except Exception:
            return False
    
    def _send_slack_alert(self, error_context: ErrorContext) -> bool:
        """å‘é€Slackå‘Šè­¦"""
        slack_config = self.notification_channels.get('slack', {})
        if not slack_config:
            return False
        
        try:
            payload = {
                'text': f"ğŸš¨ {error_context.severity.value.upper()} é”™è¯¯å‘Šè­¦",
                'attachments': [
                    {
                        'color': 'danger' if error_context.severity == ErrorSeverity.CRITICAL else 'warning',
                        'fields': [
                            {'title': 'é”™è¯¯ç±»å‹', 'value': error_context.error_type.value, 'short': True},
                            {'title': 'æ¨¡å—', 'value': error_context.module_name, 'short': True},
                            {'title': 'é”™è¯¯æ¶ˆæ¯', 'value': error_context.message, 'short': False},
                            {'title': 'æ—¶é—´', 'value': error_context.timestamp.strftime('%Y-%m-%d %H:%M:%S'), 'short': True}
                        ]
                    }
                ]
            }
            
            response = requests.post(
                slack_config['webhook_url'],
                json=payload,
                timeout=10
            )
            return response.status_code == 200
        except Exception:
            return False
    
    def _send_dingtalk_alert(self, error_context: ErrorContext) -> bool:
        """å‘é€é’‰é’‰å‘Šè­¦"""
        dingtalk_config = self.notification_channels.get('dingtalk', {})
        if not dingtalk_config:
            return False
        
        try:
            payload = {
                'msgtype': 'text',
                'text': {
                    'content': f"ğŸš¨ {error_context.severity.value.upper()} é”™è¯¯å‘Šè­¦\n\né”™è¯¯ID: {error_context.error_id}\né”™è¯¯ç±»å‹: {error_context.error_type.value}\né”™è¯¯æ¶ˆæ¯: {error_context.message}\næ¨¡å—: {error_context.module_name}\næ—¶é—´: {error_context.timestamp.strftime('%Y-%m-%d %H:%M:%S')}"
                }
            }
            
            response = requests.post(
                dingtalk_config['webhook_url'],
                json=payload,
                timeout=10
            )
            return response.status_code == 200
        except Exception:
            return False
    
    def _send_wechat_alert(self, error_context: ErrorContext) -> bool:
        """å‘é€å¾®ä¿¡å‘Šè­¦"""
        wechat_config = self.notification_channels.get('wechat', {})
        if not wechat_config:
            return False
        
        try:
            # è¿™é‡Œå¯ä»¥å®ç°ä¼ä¸šå¾®ä¿¡æœºå™¨äººçš„å‘Šè­¦é€»è¾‘
            payload = {
                'msgtype': 'text',
                'text': {
                    'content': f"ğŸš¨ {error_context.severity.value.upper()} é”™è¯¯å‘Šè­¦\n\né”™è¯¯ID: {error_context.error_id}\né”™è¯¯ç±»å‹: {error_context.error_type.value}\né”™è¯¯æ¶ˆæ¯: {error_context.message}\næ¨¡å—: {error_context.module_name}\næ—¶é—´: {error_context.timestamp.strftime('%Y-%m-%d %H:%M:%S')}"
                }
            }
            
            response = requests.post(
                wechat_config['webhook_url'],
                json=payload,
                timeout=10
            )
            return response.status_code == 200
        except Exception:
            return False
    
    def _send_log_alert(self, error_context: ErrorContext) -> bool:
        """å‘é€æ—¥å¿—å‘Šè­¦"""
        log_level = logging.ERROR if error_context.severity == ErrorSeverity.CRITICAL else logging.WARNING
        logging.log(log_level, f"é”™è¯¯å‘Šè­¦: {error_context.error_id} - {error_context.message}")
        return True


class RecoveryManager:
    """é”™è¯¯æ¢å¤ç®¡ç†å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.recovery_strategies = config.get('recovery_strategies', {})
        self.retry_policies = config.get('retry_policies', {})
        self.circuit_breakers = {}
        self.recovery_history = deque(maxlen=1000)
        self._lock = threading.RLock()
    
    def attempt_recovery(self, error_context: ErrorContext, recovery_func: Callable = None) -> Tuple[bool, Any]:
        """å°è¯•é”™è¯¯æ¢å¤"""
        with self._lock:
            recovery_key = f"{error_context.error_type.value}_{error_context.module_name}"
            
            # æ£€æŸ¥ç†”æ–­å™¨çŠ¶æ€
            if self._is_circuit_open(recovery_key):
                return False, "Circuit breaker is open"
            
            # è·å–æ¢å¤ç­–ç•¥
            strategy = self._get_recovery_strategy(error_context)
            if not strategy:
                return False, "No recovery strategy available"
            
            try:
                if strategy == RecoveryStrategy.RETRY:
                    return self._retry_recovery(error_context, recovery_func)
                elif strategy == RecoveryStrategy.FALLBACK:
                    return self._fallback_recovery(error_context, recovery_func)
                elif strategy == RecoveryStrategy.CIRCUIT_BREAKER:
                    return self._circuit_breaker_recovery(error_context, recovery_func)
                elif strategy == RecoveryStrategy.BULKHEAD:
                    return self._bulkhead_recovery(error_context, recovery_func)
                elif strategy == RecoveryStrategy.TIMEOUT:
                    return self._timeout_recovery(error_context, recovery_func)
                else:
                    return False, f"Unknown recovery strategy: {strategy}"
            except Exception as e:
                self._record_recovery_attempt(error_context, strategy, False, str(e))
                return False, str(e)
    
    def _get_recovery_strategy(self, error_context: ErrorContext) -> Optional[RecoveryStrategy]:
        """è·å–æ¢å¤ç­–ç•¥"""
        strategy_key = f"{error_context.error_type.value}_{error_context.severity.value}"
        if strategy_key in self.recovery_strategies:
            strategy_name = self.recovery_strategies[strategy_key]
            return RecoveryStrategy(strategy_name)
        
        # é»˜è®¤ç­–ç•¥
        if error_context.error_type == ErrorType.NETWORK:
            return RecoveryStrategy.RETRY
        elif error_context.error_type == ErrorType.SYSTEM:
            return RecoveryStrategy.FALLBACK
        else:
            return RecoveryStrategy.IGNORE
    
    def _is_circuit_open(self, key: str) -> bool:
        """æ£€æŸ¥ç†”æ–­å™¨æ˜¯å¦å¼€å¯"""
        if key not in self.circuit_breakers:
            return False
        
        breaker = self.circuit_breakers[key]
        if breaker['state'] == 'open':
            if time.time() - breaker['last_failure'] > breaker['timeout']:
                breaker['state'] = 'half-open'
                return False
            return True
        return False
    
    def _retry_recovery(self, error_context: ErrorContext, recovery_func: Callable) -> Tuple[bool, Any]:
        """é‡è¯•æ¢å¤"""
        retry_config = self.retry_policies.get('default', {})
        max_retries = retry_config.get('max_retries', 3)
        retry_delay = retry_config.get('retry_delay', 1.0)
        backoff_factor = retry_config.get('backoff_factor', 2.0)
        
        last_exception = None
        for attempt in range(max_retries + 1):
            try:
                if attempt > 0:
                    time.sleep(retry_delay * (backoff_factor ** (attempt - 1)))
                
                if recovery_func:
                    result = recovery_func()
                    self._record_recovery_attempt(error_context, RecoveryStrategy.RETRY, True, f"Success on attempt {attempt + 1}")
                    return True, result
                else:
                    # æ¨¡æ‹Ÿæ¢å¤æ“ä½œ
                    time.sleep(0.1)
                    self._record_recovery_attempt(error_context, RecoveryStrategy.RETRY, True, f"Simulated recovery on attempt {attempt + 1}")
                    return True, None
                    
            except Exception as e:
                last_exception = e
                continue
        
        self._record_recovery_attempt(error_context, RecoveryStrategy.RETRY, False, f"All {max_retries + 1} attempts failed")
        return False, str(last_exception)
    
    def _fallback_recovery(self, error_context: ErrorContext, recovery_func: Callable) -> Tuple[bool, Any]:
        """é™çº§æ¢å¤"""
        try:
            # æ‰§è¡Œé™çº§é€»è¾‘
            if recovery_func:
                # å°è¯•ä½¿ç”¨é™çº§å‡½æ•°
                fallback_func = getattr(recovery_func, 'fallback', None)
                if fallback_func:
                    result = fallback_func()
                    self._record_recovery_attempt(error_context, RecoveryStrategy.FALLBACK, True, "Fallback successful")
                    return True, result
            
            # é»˜è®¤é™çº§è¡Œä¸º
            logging.warning(f"æ‰§è¡Œé™çº§æ¢å¤: {error_context.error_id}")
            self._record_recovery_attempt(error_context, RecoveryStrategy.FALLBACK, True, "Default fallback executed")
            return True, None
            
        except Exception as e:
            self._record_recovery_attempt(error_context, RecoveryStrategy.FALLBACK, False, str(e))
            return False, str(e)
    
    def _circuit_breaker_recovery(self, error_context: ErrorContext, recovery_func: Callable) -> Tuple[bool, Any]:
        """ç†”æ–­å™¨æ¢å¤"""
        recovery_key = f"{error_context.error_type.value}_{error_context.module_name}"
        
        if recovery_key not in self.circuit_breakers:
            self.circuit_breakers[recovery_key] = {
                'state': 'closed',
                'failure_count': 0,
                'last_failure': 0,
                'timeout': 60  # 60ç§’è¶…æ—¶
            }
        
        breaker = self.circuit_breakers[recovery_key]
        
        try:
            if recovery_func:
                result = recovery_func()
                # æˆåŠŸï¼Œé‡ç½®ç†”æ–­å™¨
                if breaker['state'] == 'half-open':
                    breaker['state'] = 'closed'
                    breaker['failure_count'] = 0
                
                self._record_recovery_attempt(error_context, RecoveryStrategy.CIRCUIT_BREAKER, True, "Circuit breaker success")
                return True, result
            else:
                # æ¨¡æ‹Ÿç†”æ–­å™¨æ¢å¤
                time.sleep(0.1)
                self._record_recovery_attempt(error_context, RecoveryStrategy.CIRCUIT_BREAKER, True, "Circuit breaker simulated success")
                return True, None
                
        except Exception as e:
            # å¤±è´¥ï¼Œå¢åŠ å¤±è´¥è®¡æ•°
            breaker['failure_count'] += 1
            breaker['last_failure'] = time.time()
            
            # å¦‚æœå¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œå¼€å¯ç†”æ–­å™¨
            if breaker['failure_count'] >= 5:
                breaker['state'] = 'open'
            
            self._record_recovery_attempt(error_context, RecoveryStrategy.CIRCUIT_BREAKER, False, str(e))
            return False, str(e)
    
    def _bulkhead_recovery(self, error_context: ErrorContext, recovery_func: Callable) -> Tuple[bool, Any]:
        """éš”æ¿æ¨¡å¼æ¢å¤"""
        try:
            # éš”æ¿æ¨¡å¼ï¼šå°†ç³»ç»Ÿåˆ†å‰²æˆç‹¬ç«‹çš„éš”æ¿ï¼Œéš”ç¦»æ•…éšœ
            logging.info(f"æ‰§è¡Œéš”æ¿æ¨¡å¼æ¢å¤: {error_context.error_id}")
            
            # æ¨¡æ‹Ÿéš”æ¿éš”ç¦»æ“ä½œ
            time.sleep(0.1)
            
            if recovery_func:
                result = recovery_func()
                self._record_recovery_attempt(error_context, RecoveryStrategy.BULKHEAD, True, "Bulkhead isolation successful")
                return True, result
            else:
                self._record_recovery_attempt(error_context, RecoveryStrategy.BULKHEAD, True, "Bulkhead simulated recovery")
                return True, None
                
        except Exception as e:
            self._record_recovery_attempt(error_context, RecoveryStrategy.BULKHEAD, False, str(e))
            return False, str(e)
    
    def _timeout_recovery(self, error_context: ErrorContext, recovery_func: Callable) -> Tuple[bool, Any]:
        """è¶…æ—¶æ¢å¤"""
        timeout = self.config.get('default_timeout', 30.0)
        
        try:
            if recovery_func:
                # åœ¨æŒ‡å®šè¶…æ—¶æ—¶é—´å†…å°è¯•æ‰§è¡Œ
                result = asyncio.wait_for(
                    asyncio.get_event_loop().run_in_executor(None, recovery_func),
                    timeout=timeout
                )
                self._record_recovery_attempt(error_context, RecoveryStrategy.TIMEOUT, True, "Timeout recovery successful")
                return True, result
            else:
                # æ¨¡æ‹Ÿè¶…æ—¶æ¢å¤
                time.sleep(min(0.1, timeout))
                self._record_recovery_attempt(error_context, RecoveryStrategy.TIMEOUT, True, "Timeout simulated recovery")
                return True, None
                
        except asyncio.TimeoutError:
            self._record_recovery_attempt(error_context, RecoveryStrategy.TIMEOUT, False, "Operation timed out")
            return False, "Operation timed out"
        except Exception as e:
            self._record_recovery_attempt(error_context, RecoveryStrategy.TIMEOUT, False, str(e))
            return False, str(e)
    
    def _record_recovery_attempt(self, error_context: ErrorContext, strategy: RecoveryStrategy, success: bool, message: str):
        """è®°å½•æ¢å¤å°è¯•"""
        self.recovery_history.append({
            'timestamp': time.time(),
            'error_id': error_context.error_id,
            'strategy': strategy.value,
            'success': success,
            'message': message
        })


class DatabaseManager:
    """æ•°æ®åº“ç®¡ç†å™¨"""
    
    def __init__(self, db_path: str = "error_logs.db"):
        self.db_path = db_path
        self._lock = threading.RLock()
        self._init_database()
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        with self._lock:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # åˆ›å»ºé”™è¯¯æ—¥å¿—è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS error_logs (
                    error_id TEXT PRIMARY KEY,
                    timestamp TEXT NOT NULL,
                    error_type TEXT NOT NULL,
                    severity TEXT NOT NULL,
                    message TEXT NOT NULL,
                    exception_type TEXT NOT NULL,
                    stack_trace TEXT NOT NULL,
                    module_name TEXT NOT NULL,
                    function_name TEXT NOT NULL,
                    line_number INTEGER NOT NULL,
                    user_id TEXT,
                    session_id TEXT,
                    request_id TEXT,
                    service_name TEXT,
                    host_name TEXT,
                    process_id INTEGER,
                    thread_id INTEGER,
                    memory_usage REAL,
                    cpu_usage REAL,
                    additional_data TEXT,
                    custom_fields TEXT,
                    status TEXT DEFAULT 'open',
                    resolved_at TEXT,
                    resolution_time REAL,
                    recovery_attempts INTEGER DEFAULT 0,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    updated_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # åˆ›å»ºé”™è¯¯ç»Ÿè®¡è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS error_statistics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    date TEXT NOT NULL,
                    total_errors INTEGER DEFAULT 0,
                    errors_by_type TEXT,
                    errors_by_severity TEXT,
                    errors_by_module TEXT,
                    recurring_errors INTEGER DEFAULT 0,
                    resolved_errors INTEGER DEFAULT 0,
                    avg_resolution_time REAL DEFAULT 0.0,
                    affected_users INTEGER DEFAULT 0,
                    system_impact_score REAL DEFAULT 0.0,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # åˆ›å»ºå‘Šè­¦å†å²è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS alert_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp REAL NOT NULL,
                    error_id TEXT NOT NULL,
                    channels TEXT NOT NULL,
                    success_count INTEGER NOT NULL,
                    total_count INTEGER NOT NULL,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # åˆ›å»ºæ¢å¤å†å²è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS recovery_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp REAL NOT NULL,
                    error_id TEXT NOT NULL,
                    strategy TEXT NOT NULL,
                    success BOOLEAN NOT NULL,
                    message TEXT NOT NULL,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # åˆ›å»ºç´¢å¼•
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_error_logs_timestamp ON error_logs(timestamp)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_error_logs_type ON error_logs(error_type)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_error_logs_severity ON error_logs(severity)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_error_logs_status ON error_logs(status)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_error_statistics_date ON error_statistics(date)')
            
            conn.commit()
            conn.close()
    
    def save_error_log(self, error_context: ErrorContext) -> bool:
        """ä¿å­˜é”™è¯¯æ—¥å¿—"""
        try:
            with self._lock:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                cursor.execute('''
                    INSERT OR REPLACE INTO error_logs (
                        error_id, timestamp, error_type, severity, message, exception_type,
                        stack_trace, module_name, function_name, line_number, user_id,
                        session_id, request_id, service_name, host_name, process_id,
                        thread_id, memory_usage, cpu_usage, additional_data, custom_fields
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    error_context.error_id,
                    error_context.timestamp.isoformat(),
                    error_context.error_type.value,
                    error_context.severity.value,
                    error_context.message,
                    error_context.exception_type,
                    error_context.stack_trace,
                    error_context.module_name,
                    error_context.function_name,
                    error_context.line_number,
                    error_context.user_id,
                    error_context.session_id,
                    error_context.request_id,
                    error_context.service_name,
                    error_context.host_name,
                    error_context.process_id,
                    error_context.thread_id,
                    error_context.memory_usage,
                    error_context.cpu_usage,
                    json.dumps(error_context.additional_data) if error_context.additional_data else None,
                    json.dumps(error_context.custom_fields) if error_context.custom_fields else None
                ))
                
                conn.commit()
                conn.close()
                return True
        except Exception as e:
            logging.error(f"ä¿å­˜é”™è¯¯æ—¥å¿—å¤±è´¥: {str(e)}")
            return False
    
    def update_error_status(self, error_id: str, status: ErrorStatus, resolution_time: float = None) -> bool:
        """æ›´æ–°é”™è¯¯çŠ¶æ€"""
        try:
            with self._lock:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                update_data = {
                    'status': status.value,
                    'updated_at': datetime.now().isoformat()
                }
                
                if status in [ErrorStatus.RESOLVED, ErrorStatus.CLOSED]:
                    update_data['resolved_at'] = datetime.now().isoformat()
                    if resolution_time:
                        update_data['resolution_time'] = resolution_time
                
                set_clause = ', '.join([f"{k} = ?" for k in update_data.keys()])
                values = list(update_data.values()) + [error_id]
                
                cursor.execute(f'UPDATE error_logs SET {set_clause} WHERE error_id = ?', values)
                
                conn.commit()
                conn.close()
                return cursor.rowcount > 0
        except Exception as e:
            logging.error(f"æ›´æ–°é”™è¯¯çŠ¶æ€å¤±è´¥: {str(e)}")
            return False
    
    def get_error_logs(self, filters: Dict[str, Any] = None, limit: int = 100) -> List[Dict[str, Any]]:
        """è·å–é”™è¯¯æ—¥å¿—"""
        try:
            with self._lock:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                where_conditions = []
                values = []
                
                if filters:
                    if 'error_type' in filters:
                        where_conditions.append('error_type = ?')
                        values.append(filters['error_type'])
                    if 'severity' in filters:
                        where_conditions.append('severity = ?')
                        values.append(filters['severity'])
                    if 'status' in filters:
                        where_conditions.append('status = ?')
                        values.append(filters['status'])
                    if 'start_date' in filters:
                        where_conditions.append('timestamp >= ?')
                        values.append(filters['start_date'])
                    if 'end_date' in filters:
                        where_conditions.append('timestamp <= ?')
                        values.append(filters['end_date'])
                
                where_clause = 'WHERE ' + ' AND '.join(where_conditions) if where_conditions else ''
                query = f'''
                    SELECT * FROM error_logs 
                    {where_clause}
                    ORDER BY timestamp DESC 
                    LIMIT ?
                '''
                values.append(limit)
                
                cursor.execute(query, values)
                columns = [desc[0] for desc in cursor.description]
                results = [dict(zip(columns, row)) for row in cursor.fetchall()]
                
                conn.close()
                return results
        except Exception as e:
            logging.error(f"è·å–é”™è¯¯æ—¥å¿—å¤±è´¥: {str(e)}")
            return []
    
    def save_statistics(self, statistics: ErrorStatistics) -> bool:
        """ä¿å­˜ç»Ÿè®¡æ•°æ®"""
        try:
            with self._lock:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                cursor.execute('''
                    INSERT INTO error_statistics (
                        date, total_errors, errors_by_type, errors_by_severity,
                        errors_by_module, recurring_errors, resolved_errors,
                        avg_resolution_time, affected_users, system_impact_score
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    datetime.now().strftime('%Y-%m-%d'),
                    statistics.total_errors,
                    json.dumps({k.value: v for k, v in statistics.errors_by_type.items()}),
                    json.dumps({k.value: v for k, v in statistics.errors_by_severity.items()}),
                    json.dumps(statistics.errors_by_module),
                    statistics.recurring_errors,
                    statistics.resolved_errors,
                    statistics.avg_resolution_time,
                    statistics.affected_users,
                    statistics.system_impact_score
                ))
                
                conn.commit()
                conn.close()
                return True
        except Exception as e:
            logging.error(f"ä¿å­˜ç»Ÿè®¡æ•°æ®å¤±è´¥: {str(e)}")
            return False


class ErrorLogger:
    """
    L3é”™è¯¯æ—¥å¿—è®°å½•å™¨ä¸»ç±»
    
    æä¾›å®Œæ•´çš„é”™è¯¯æ—¥å¿—è®°å½•ã€å¤„ç†å’Œåˆ†æåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
    - å¼‚å¸¸æ•è·å’Œåˆ†ç±»è®°å½•
    - é”™è¯¯å †æ ˆè·Ÿè¸ªå’Œä¸Šä¸‹æ–‡ä¿¡æ¯
    - é”™è¯¯ç»Ÿè®¡å’Œåˆ†æ
    - é”™è¯¯å‘Šè­¦å’Œé€šçŸ¥æœºåˆ¶
    - é”™è¯¯æ¢å¤å’Œé‡è¯•æ—¥å¿—
    - é”™è¯¯è§£å†³å’Œå…³é—­æµç¨‹
    - å¼‚æ­¥é”™è¯¯æ—¥å¿—å¤„ç†
    
    ä½¿ç”¨ç¤ºä¾‹:
        ```python
        # åˆå§‹åŒ–é”™è¯¯æ—¥å¿—è®°å½•å™¨
        error_logger = ErrorLogger(config={
            'database_path': 'error_logs.db',
            'enable_alerts': True,
            'alert_rules': {
                'system_critical': {'enabled': True},
                'business_high': {'enabled': True}
            },
            'notification_channels': {
                'email': {
                    'smtp_server': 'smtp.example.com',
                    'smtp_port': 587,
                    'from': 'alerts@example.com',
                    'to': ['admin@example.com'],
                    'username': 'user@example.com',
                    'password': 'password'
                }
            }
        })
        
        # ä½¿ç”¨è£…é¥°å™¨æ•è·å¼‚å¸¸
        @error_logger.error_handler
        def risky_function():
            # å¯èƒ½æŠ›å‡ºå¼‚å¸¸çš„ä»£ç 
            pass
        
        # æ‰‹åŠ¨è®°å½•é”™è¯¯
        try:
            # ä¸šåŠ¡ä»£ç 
            pass
        except Exception as e:
            error_logger.log_error(
                error_type=ErrorType.BUSINESS,
                severity=ErrorSeverity.HIGH,
                message="ä¸šåŠ¡é€»è¾‘é”™è¯¯",
                exception=e
            )
        
        # è·å–é”™è¯¯ç»Ÿè®¡
        stats = error_logger.get_statistics()
        print(f"æ€»é”™è¯¯æ•°: {stats.total_errors}")
        ```
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–é”™è¯¯æ—¥å¿—è®°å½•å™¨
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«æ•°æ®åº“è·¯å¾„ã€å‘Šè­¦é…ç½®ç­‰
        """
        self.config = config or {}
        self.database_path = self.config.get('database_path', 'error_logs.db')
        self.enable_alerts = self.config.get('enable_alerts', True)
        self.enable_recovery = self.config.get('enable_recovery', True)
        self.enable_statistics = self.config.get('enable_statistics', True)
        self.async_processing = self.config.get('async_processing', True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.db_manager = DatabaseManager(self.database_path)
        self.alert_manager = AlertManager(self.config)
        self.recovery_manager = RecoveryManager(self.config)
        
        # å†…å­˜ç¼“å­˜
        self.error_cache = deque(maxlen=10000)
        self.statistics_cache = {}
        self._lock = threading.RLock()
        
        # å¼‚æ­¥å¤„ç†
        self.async_queue = asyncio.Queue(maxsize=1000)
        self.processing_tasks = []
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.start_time = time.time()
        self.total_errors_logged = 0
        self.total_recoveries_attempted = 0
        self.total_alerts_sent = 0
        
        # å¯åŠ¨å¼‚æ­¥å¤„ç†
        if self.async_processing:
            self._start_async_processing()
    
    def _start_async_processing(self):
        """å¯åŠ¨å¼‚æ­¥å¤„ç†ä»»åŠ¡"""
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        # å¯åŠ¨å¼‚æ­¥å¤„ç†ä»»åŠ¡
        task = loop.create_task(self._async_processing_loop())
        self.processing_tasks.append(task)
    
    async def _async_processing_loop(self):
        """å¼‚æ­¥å¤„ç†å¾ªç¯"""
        while True:
            try:
                # ä»é˜Ÿåˆ—è·å–å¾…å¤„ç†é¡¹ç›®
                item = await asyncio.wait_for(self.async_queue.get(), timeout=1.0)
                
                if item['type'] == 'save_error':
                    await self._async_save_error(item['data'])
                elif item['type'] == 'send_alert':
                    await self._async_send_alert(item['data'])
                elif item['type'] == 'update_statistics':
                    await self._async_update_statistics(item['data'])
                
                self.async_queue.task_done()
                
            except asyncio.TimeoutError:
                # é˜Ÿåˆ—ä¸ºç©ºï¼Œç»§ç»­å¾ªç¯
                continue
            except Exception as e:
                logging.error(f"å¼‚æ­¥å¤„ç†é”™è¯¯: {str(e)}")
    
    async def _async_save_error(self, error_context: ErrorContext):
        """å¼‚æ­¥ä¿å­˜é”™è¯¯"""
        try:
            success = self.db_manager.save_error_log(error_context)
            if success:
                with self._lock:
                    self.error_cache.append(error_context)
                    self.total_errors_logged += 1
        except Exception as e:
            logging.error(f"å¼‚æ­¥ä¿å­˜é”™è¯¯å¤±è´¥: {str(e)}")
    
    async def _async_send_alert(self, error_context: ErrorContext):
        """å¼‚æ­¥å‘é€å‘Šè­¦"""
        try:
            if self.alert_manager.should_alert(error_context):
                channels = self._get_alert_channels(error_context)
                success = self.alert_manager.send_alert(error_context, channels)
                if success:
                    with self._lock:
                        self.total_alerts_sent += 1
        except Exception as e:
            logging.error(f"å¼‚æ­¥å‘é€å‘Šè­¦å¤±è´¥: {str(e)}")
    
    async def _async_update_statistics(self, statistics: ErrorStatistics):
        """å¼‚æ­¥æ›´æ–°ç»Ÿè®¡"""
        try:
            self.db_manager.save_statistics(statistics)
            with self._lock:
                self.statistics_cache = asdict(statistics)
        except Exception as e:
            logging.error(f"å¼‚æ­¥æ›´æ–°ç»Ÿè®¡å¤±è´¥: {str(e)}")
    
    def _get_alert_channels(self, error_context: ErrorContext) -> List[AlertChannel]:
        """è·å–å‘Šè­¦æ¸ é“"""
        channels = [AlertChannel.LOG]  # é»˜è®¤æ€»æ˜¯è®°å½•æ—¥å¿—
        
        if not self.enable_alerts:
            return channels
        
        # æ ¹æ®é”™è¯¯ç±»å‹å’Œä¸¥é‡ç¨‹åº¦ç¡®å®šå‘Šè­¦æ¸ é“
        if error_context.severity == ErrorSeverity.CRITICAL:
            channels.extend([AlertChannel.EMAIL, AlertChannel.SLACK, AlertChannel.DINGTALK])
        elif error_context.severity == ErrorSeverity.HIGH:
            channels.extend([AlertChannel.EMAIL, AlertChannel.SLACK])
        elif error_context.error_type == ErrorType.SYSTEM:
            channels.append(AlertChannel.EMAIL)
        
        return channels
    
    def _create_error_context(self, 
                            error_type: ErrorType,
                            severity: ErrorSeverity,
                            message: str,
                            exception: Exception = None,
                            additional_data: Dict[str, Any] = None,
                            custom_fields: Dict[str, Any] = None) -> ErrorContext:
        """åˆ›å»ºé”™è¯¯ä¸Šä¸‹æ–‡"""
        
        # è·å–è°ƒç”¨æ ˆä¿¡æ¯
        stack = traceback.extract_tb(exception.__traceback__ if exception else None)
        if stack:
            frame = stack[-1]  # æœ€æ–°çš„æ ˆå¸§
            module_name = frame.filename
            function_name = frame.name
            line_number = frame.lineno
            stack_trace = ''.join(traceback.format_tb(exception.__traceback__))
        else:
            # ä»å½“å‰è°ƒç”¨æ ˆè·å–ä¿¡æ¯
            frame = sys._getframe(2)
            module_name = frame.f_code.co_filename
            function_name = frame.f_code.co_name
            line_number = frame.f_lineno
            stack_trace = ''.join(traceback.format_stack())
        
        # è·å–ç³»ç»Ÿä¿¡æ¯
        process = psutil.Process()
        memory_info = process.memory_info()
        cpu_percent = process.cpu_percent()
        
        return ErrorContext(
            error_id=str(uuid.uuid4()),
            timestamp=datetime.now(),
            error_type=error_type,
            severity=severity,
            message=message,
            exception_type=type(exception).__name__ if exception else 'Unknown',
            stack_trace=stack_trace,
            module_name=os.path.basename(module_name),
            function_name=function_name,
            line_number=line_number,
            host_name=os.uname().nodename,
            process_id=os.getpid(),
            thread_id=threading.get_ident(),
            memory_usage=memory_info.rss / 1024 / 1024,  # MB
            cpu_usage=cpu_percent,
            additional_data=additional_data,
            custom_fields=custom_fields
        )
    
    def error_handler(self, 
                     error_type: ErrorType = ErrorType.UNKNOWN,
                     severity: ErrorSeverity = ErrorSeverity.MEDIUM,
                     reraise: bool = True,
                     recovery_func: Callable = None,
                     additional_data: Dict[str, Any] = None,
                     custom_fields: Dict[str, Any] = None):
        """
        é”™è¯¯å¤„ç†è£…é¥°å™¨
        
        Args:
            error_type: é”™è¯¯ç±»å‹
            severity: é”™è¯¯ä¸¥é‡ç¨‹åº¦
            reraise: æ˜¯å¦é‡æ–°æŠ›å‡ºå¼‚å¸¸
            recovery_func: æ¢å¤å‡½æ•°
            additional_data: é¢å¤–æ•°æ®
            custom_fields: è‡ªå®šä¹‰å­—æ®µ
            
        Returns:
            è£…é¥°å™¨å‡½æ•°
        """
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    # è®°å½•é”™è¯¯
                    self.log_error(
                        error_type=error_type,
                        severity=severity,
                        message=f"å‡½æ•° {func.__name__} æ‰§è¡Œå¤±è´¥: {str(e)}",
                        exception=e,
                        additional_data=additional_data,
                        custom_fields=custom_fields
                    )
                    
                    # å°è¯•æ¢å¤
                    if self.enable_recovery and recovery_func:
                        success, result = self.attempt_recovery(error_type, recovery_func)
                        if success:
                            return result
                    
                    # é‡æ–°æŠ›å‡ºå¼‚å¸¸
                    if reraise:
                        raise
                    
                    return None
            return wrapper
        return decorator
    
    def log_error(self,
                 error_type: ErrorType,
                 severity: ErrorSeverity,
                 message: str,
                 exception: Exception = None,
                 additional_data: Dict[str, Any] = None,
                 custom_fields: Dict[str, Any] = None,
                 user_id: str = None,
                 session_id: str = None,
                 request_id: str = None,
                 service_name: str = None) -> str:
        """
        è®°å½•é”™è¯¯æ—¥å¿—
        
        Args:
            error_type: é”™è¯¯ç±»å‹
            severity: é”™è¯¯ä¸¥é‡ç¨‹åº¦
            message: é”™è¯¯æ¶ˆæ¯
            exception: å¼‚å¸¸å¯¹è±¡
            additional_data: é¢å¤–æ•°æ®
            custom_fields: è‡ªå®šä¹‰å­—æ®µ
            user_id: ç”¨æˆ·ID
            session_id: ä¼šè¯ID
            request_id: è¯·æ±‚ID
            service_name: æœåŠ¡åç§°
            
        Returns:
            é”™è¯¯ID
        """
        try:
            # åˆ›å»ºé”™è¯¯ä¸Šä¸‹æ–‡
            error_context = self._create_error_context(
                error_type=error_type,
                severity=severity,
                message=message,
                exception=exception,
                additional_data=additional_data,
                custom_fields=custom_fields
            )
            
            # è®¾ç½®ç”¨æˆ·å’Œä¼šè¯ä¿¡æ¯
            error_context.user_id = user_id
            error_context.session_id = session_id
            error_context.request_id = request_id
            error_context.service_name = service_name
            
            # å¼‚æ­¥å¤„ç†
            if self.async_processing:
                try:
                    loop = asyncio.get_event_loop()
                    loop.create_task(self.async_queue.put({
                        'type': 'save_error',
                        'data': error_context
                    }))
                    
                    # å¦‚æœéœ€è¦å‘Šè­¦ï¼Œä¹Ÿå¼‚æ­¥å¤„ç†
                    if self.enable_alerts:
                        loop.create_task(self.async_queue.put({
                            'type': 'send_alert',
                            'data': error_context
                        }))
                        
                except Exception as e:
                    logging.error(f"å¼‚æ­¥å¤„ç†é”™è¯¯: {str(e)}")
                    # é™çº§åˆ°åŒæ­¥å¤„ç†
                    self._sync_save_error(error_context)
            else:
                # åŒæ­¥å¤„ç†
                self._sync_save_error(error_context)
                if self.enable_alerts:
                    self._sync_send_alert(error_context)
            
            return error_context.error_id
            
        except Exception as e:
            logging.error(f"è®°å½•é”™è¯¯æ—¥å¿—å¤±è´¥: {str(e)}")
            return ""
    
    def _sync_save_error(self, error_context: ErrorContext):
        """åŒæ­¥ä¿å­˜é”™è¯¯"""
        try:
            success = self.db_manager.save_error_log(error_context)
            if success:
                with self._lock:
                    self.error_cache.append(error_context)
                    self.total_errors_logged += 1
        except Exception as e:
            logging.error(f"åŒæ­¥ä¿å­˜é”™è¯¯å¤±è´¥: {str(e)}")
    
    def _sync_send_alert(self, error_context: ErrorContext):
        """åŒæ­¥å‘é€å‘Šè­¦"""
        try:
            if self.alert_manager.should_alert(error_context):
                channels = self._get_alert_channels(error_context)
                success = self.alert_manager.send_alert(error_context, channels)
                if success:
                    with self._lock:
                        self.total_alerts_sent += 1
        except Exception as e:
            logging.error(f"åŒæ­¥å‘é€å‘Šè­¦å¤±è´¥: {str(e)}")
    
    def attempt_recovery(self, 
                        error_type: ErrorType, 
                        recovery_func: Callable,
                        error_context: ErrorContext = None) -> Tuple[bool, Any]:
        """
        å°è¯•é”™è¯¯æ¢å¤
        
        Args:
            error_type: é”™è¯¯ç±»å‹
            recovery_func: æ¢å¤å‡½æ•°
            error_context: é”™è¯¯ä¸Šä¸‹æ–‡
            
        Returns:
            (æ˜¯å¦æˆåŠŸ, ç»“æœæˆ–é”™è¯¯æ¶ˆæ¯)
        """
        try:
            with self._lock:
                self.total_recoveries_attempted += 1
            
            # å¦‚æœæ²¡æœ‰æä¾›é”™è¯¯ä¸Šä¸‹æ–‡ï¼Œåˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„
            if error_context is None:
                error_context = self._create_error_context(
                    error_type=error_type,
                    severity=ErrorSeverity.MEDIUM,
                    message="Recovery attempt"
                )
            
            success, result = self.recovery_manager.attempt_recovery(error_context, recovery_func)
            
            if success:
                logging.info(f"é”™è¯¯æ¢å¤æˆåŠŸ: {error_context.error_id}")
            else:
                logging.warning(f"é”™è¯¯æ¢å¤å¤±è´¥: {error_context.error_id} - {result}")
            
            return success, result
            
        except Exception as e:
            logging.error(f"é”™è¯¯æ¢å¤è¿‡ç¨‹å¼‚å¸¸: {str(e)}")
            return False, str(e)
    
    def resolve_error(self, error_id: str, resolution_notes: str = None) -> bool:
        """
        è§£å†³é”™è¯¯
        
        Args:
            error_id: é”™è¯¯ID
            resolution_notes: è§£å†³å¤‡æ³¨
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            # è®¡ç®—è§£å†³æ—¶é—´
            error_log = self.db_manager.get_error_logs({'error_id': error_id}, 1)
            if not error_log:
                return False
            
            error_log = error_log[0]
            created_time = datetime.fromisoformat(error_log['timestamp'])
            resolution_time = (datetime.now() - created_time).total_seconds()
            
            # æ›´æ–°æ•°æ®åº“
            success = self.db_manager.update_error_status(
                error_id=error_id,
                status=ErrorStatus.RESOLVED,
                resolution_time=resolution_time
            )
            
            if success:
                logging.info(f"é”™è¯¯å·²è§£å†³: {error_id}")
                if resolution_notes:
                    logging.info(f"è§£å†³å¤‡æ³¨: {resolution_notes}")
            
            return success
            
        except Exception as e:
            logging.error(f"è§£å†³é”™è¯¯å¤±è´¥: {str(e)}")
            return False
    
    def close_error(self, error_id: str, close_notes: str = None) -> bool:
        """
        å…³é—­é”™è¯¯
        
        Args:
            error_id: é”™è¯¯ID
            close_notes: å…³é—­å¤‡æ³¨
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            success = self.db_manager.update_error_status(
                error_id=error_id,
                status=ErrorStatus.CLOSED
            )
            
            if success:
                logging.info(f"é”™è¯¯å·²å…³é—­: {error_id}")
                if close_notes:
                    logging.info(f"å…³é—­å¤‡æ³¨: {close_notes}")
            
            return success
            
        except Exception as e:
            logging.error(f"å…³é—­é”™è¯¯å¤±è´¥: {str(e)}")
            return False
    
    def ignore_error(self, error_id: str, ignore_reason: str = None) -> bool:
        """
        å¿½ç•¥é”™è¯¯
        
        Args:
            error_id: é”™è¯¯ID
            ignore_reason: å¿½ç•¥åŸå› 
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            success = self.db_manager.update_error_status(
                error_id=error_id,
                status=ErrorStatus.IGNORED
            )
            
            if success:
                logging.info(f"é”™è¯¯å·²å¿½ç•¥: {error_id}")
                if ignore_reason:
                    logging.info(f"å¿½ç•¥åŸå› : {ignore_reason}")
            
            return success
            
        except Exception as e:
            logging.error(f"å¿½ç•¥é”™è¯¯å¤±è´¥: {str(e)}")
            return False
    
    def get_statistics(self, hours: int = 24) -> ErrorStatistics:
        """
        è·å–é”™è¯¯ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            hours: ç»Ÿè®¡æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            é”™è¯¯ç»Ÿè®¡å¯¹è±¡
        """
        try:
            end_time = datetime.now()
            start_time = end_time - timedelta(hours=hours)
            
            # è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„é”™è¯¯æ—¥å¿—
            filters = {
                'start_date': start_time.isoformat(),
                'end_date': end_time.isoformat()
            }
            error_logs = self.db_manager.get_error_logs(filters, limit=10000)
            
            # åˆå§‹åŒ–ç»Ÿè®¡å¯¹è±¡
            stats = ErrorStatistics()
            stats.total_errors = len(error_logs)
            
            # ç»Ÿè®¡é”™è¯¯ç±»å‹
            for log in error_logs:
                error_type = ErrorType(log['error_type'])
                severity = ErrorSeverity(log['severity'])
                module = log['module_name']
                timestamp = datetime.fromisoformat(log['timestamp'])
                hour = timestamp.hour
                
                stats.errors_by_type[error_type] += 1
                stats.errors_by_severity[severity] += 1
                stats.errors_by_module[module] = stats.errors_by_module.get(module, 0) + 1
                stats.errors_by_hour[hour] = stats.errors_by_hour.get(hour, 0) + 1
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤é”™è¯¯
                if log['status'] == 'recurring':
                    stats.recurring_errors += 1
                
                # æ£€æŸ¥æ˜¯å¦å·²è§£å†³
                if log['status'] in ['resolved', 'closed']:
                    stats.resolved_errors += 1
                    if log['resolution_time']:
                        stats.avg_resolution_time += log['resolution_time']
            
            # è®¡ç®—å¹³å‡è§£å†³æ—¶é—´
            if stats.resolved_errors > 0:
                stats.avg_resolution_time /= stats.resolved_errors
            
            # è®¡ç®—é”™è¯¯é¢‘ç‡
            error_frequency = defaultdict(int)
            for log in error_logs:
                key = f"{log['error_type']}_{log['module_name']}_{log['function_name']}"
                error_frequency[key] += 1
            
            stats.error_frequency = dict(error_frequency)
            
            # è®¡ç®—å³°å€¼æ—¶é—´
            if stats.errors_by_hour:
                peak_hour = max(stats.errors_by_hour, key=stats.errors_by_hour.get)
                stats.peak_error_time = datetime.now().replace(hour=peak_hour, minute=0, second=0, microsecond=0)
            
            # è®¡ç®—ç³»ç»Ÿå½±å“åˆ†æ•°
            impact_score = 0
            for severity, count in stats.errors_by_severity.items():
                if severity == ErrorSeverity.CRITICAL:
                    impact_score += count * 10
                elif severity == ErrorSeverity.HIGH:
                    impact_score += count * 5
                elif severity == ErrorSeverity.MEDIUM:
                    impact_score += count * 2
                else:
                    impact_score += count * 1
            
            stats.system_impact_score = impact_score / max(stats.total_errors, 1)
            
            # å¼‚æ­¥ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            if self.enable_statistics:
                try:
                    loop = asyncio.get_event_loop()
                    loop.create_task(self.async_queue.put({
                        'type': 'update_statistics',
                        'data': stats
                    }))
                except Exception:
                    # é™çº§åˆ°åŒæ­¥ä¿å­˜
                    self.db_manager.save_statistics(stats)
            
            return stats
            
        except Exception as e:
            logging.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return ErrorStatistics()
    
    def get_error_logs(self, 
                      error_type: ErrorType = None,
                      severity: ErrorSeverity = None,
                      status: ErrorStatus = None,
                      start_date: datetime = None,
                      end_date: datetime = None,
                      limit: int = 100) -> List[Dict[str, Any]]:
        """
        è·å–é”™è¯¯æ—¥å¿—
        
        Args:
            error_type: é”™è¯¯ç±»å‹è¿‡æ»¤
            severity: ä¸¥é‡ç¨‹åº¦è¿‡æ»¤
            status: çŠ¶æ€è¿‡æ»¤
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            limit: è¿”å›æ•°é‡é™åˆ¶
            
        Returns:
            é”™è¯¯æ—¥å¿—åˆ—è¡¨
        """
        try:
            filters = {}
            
            if error_type:
                filters['error_type'] = error_type.value
            if severity:
                filters['severity'] = severity.value
            if status:
                filters['status'] = status.value
            if start_date:
                filters['start_date'] = start_date.isoformat()
            if end_date:
                filters['end_date'] = end_date.isoformat()
            
            return self.db_manager.get_error_logs(filters, limit)
            
        except Exception as e:
            logging.error(f"è·å–é”™è¯¯æ—¥å¿—å¤±è´¥: {str(e)}")
            return []
    
    def get_recent_errors(self, count: int = 50) -> List[ErrorContext]:
        """
        è·å–æœ€è¿‘çš„é”™è¯¯
        
        Args:
            count: è¿”å›æ•°é‡
            
        Returns:
            é”™è¯¯ä¸Šä¸‹æ–‡åˆ—è¡¨
        """
        try:
            with self._lock:
                return list(self.error_cache)[-count:]
        except Exception as e:
            logging.error(f"è·å–æœ€è¿‘é”™è¯¯å¤±è´¥: {str(e)}")
            return []
    
    def get_critical_errors(self, hours: int = 1) -> List[ErrorContext]:
        """
        è·å–ä¸¥é‡é”™è¯¯
        
        Args:
            hours: æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            ä¸¥é‡é”™è¯¯åˆ—è¡¨
        """
        try:
            cutoff_time = datetime.now() - timedelta(hours=hours)
            critical_errors = []
            
            with self._lock:
                for error in reversed(self.error_cache):
                    if (error.timestamp >= cutoff_time and 
                        error.severity in [ErrorSeverity.CRITICAL, ErrorSeverity.HIGH]):
                        critical_errors.append(error)
            
            return critical_errors
        except Exception as e:
            logging.error(f"è·å–ä¸¥é‡é”™è¯¯å¤±è´¥: {str(e)}")
            return []
    
    def get_system_health(self) -> Dict[str, Any]:
        """
        è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€
        
        Returns:
            ç³»ç»Ÿå¥åº·çŠ¶æ€å­—å…¸
        """
        try:
            uptime = time.time() - self.start_time
            
            # è·å–æœ€è¿‘çš„ç»Ÿè®¡ä¿¡æ¯
            recent_stats = self.get_statistics(hours=1)
            
            health_status = {
                'uptime_seconds': uptime,
                'total_errors_logged': self.total_errors_logged,
                'total_recoveries_attempted': self.total_recoveries_attempted,
                'total_alerts_sent': self.total_alerts_sent,
                'error_rate_per_hour': recent_stats.total_errors,
                'system_impact_score': recent_stats.system_impact_score,
                'critical_errors_last_hour': len(self.get_critical_errors(1)),
                'database_connected': True,
                'async_processing_active': len(self.processing_tasks) > 0,
                'cache_size': len(self.error_cache),
                'memory_usage_mb': psutil.Process().memory_info().rss / 1024 / 1024,
                'cpu_usage_percent': psutil.Process().cpu_percent()
            }
            
            # è®¡ç®—å¥åº·åˆ†æ•°
            health_score = 100.0
            if health_status['error_rate_per_hour'] > 100:
                health_score -= 20
            elif health_status['error_rate_per_hour'] > 50:
                health_score -= 10
            
            if health_status['critical_errors_last_hour'] > 5:
                health_score -= 30
            elif health_status['critical_errors_last_hour'] > 0:
                health_score -= 10
            
            if health_status['system_impact_score'] > 50:
                health_score -= 20
            elif health_status['system_impact_score'] > 20:
                health_score -= 10
            
            health_status['health_score'] = max(health_score, 0)
            
            # ç¡®å®šå¥åº·çŠ¶æ€
            if health_status['health_score'] >= 80:
                health_status['status'] = 'healthy'
            elif health_status['health_score'] >= 60:
                health_status['status'] = 'warning'
            else:
                health_status['status'] = 'critical'
            
            return health_status
            
        except Exception as e:
            logging.error(f"è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€å¤±è´¥: {str(e)}")
            return {'status': 'error', 'message': str(e)}
    
    def cleanup_old_logs(self, days: int = 30) -> int:
        """
        æ¸…ç†æ—§æ—¥å¿—
        
        Args:
            days: ä¿ç•™å¤©æ•°
            
        Returns:
            æ¸…ç†çš„è®°å½•æ•°
        """
        try:
            cutoff_date = datetime.now() - timedelta(days=days)
            
            with self._lock:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                # åˆ é™¤æ—§è®°å½•
                cursor.execute(
                    'DELETE FROM error_logs WHERE timestamp < ?',
                    (cutoff_date.isoformat(),)
                )
                deleted_count = cursor.rowcount
                
                cursor.execute(
                    'DELETE FROM error_statistics WHERE date < ?',
                    (cutoff_date.strftime('%Y-%m-%d'),)
                )
                deleted_stats_count = cursor.rowcount
                
                cursor.execute(
                    'DELETE FROM alert_history WHERE timestamp < ?',
                    (time.time() - (days * 24 * 3600),)
                )
                deleted_alerts_count = cursor.rowcount
                
                cursor.execute(
                    'DELETE FROM recovery_history WHERE timestamp < ?',
                    (time.time() - (days * 24 * 3600),)
                )
                deleted_recovery_count = cursor.rowcount
                
                conn.commit()
                conn.close()
                
                total_deleted = deleted_count + deleted_stats_count + deleted_alerts_count + deleted_recovery_count
                
                logging.info(f"æ¸…ç†å®Œæˆï¼Œåˆ é™¤ {total_deleted} æ¡æ—§è®°å½•")
                return total_deleted
                
        except Exception as e:
            logging.error(f"æ¸…ç†æ—§æ—¥å¿—å¤±è´¥: {str(e)}")
            return 0
    
    def export_error_report(self, output_file: str, hours: int = 24) -> bool:
        """
        å¯¼å‡ºé”™è¯¯æŠ¥å‘Š
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            hours: ç»Ÿè®¡æ—¶é—´èŒƒå›´
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            # è·å–ç»Ÿè®¡æ•°æ®
            stats = self.get_statistics(hours)
            error_logs = self.get_error_logs(limit=1000)
            health_status = self.get_system_health()
            
            # ç”ŸæˆæŠ¥å‘Š
            report = {
                'report_info': {
                    'generated_at': datetime.now().isoformat(),
                    'time_range_hours': hours,
                    'report_version': '1.0.0'
                },
                'summary': {
                    'total_errors': stats.total_errors,
                    'critical_errors': stats.errors_by_severity.get(ErrorSeverity.CRITICAL, 0),
                    'high_errors': stats.errors_by_severity.get(ErrorSeverity.HIGH, 0),
                    'resolved_errors': stats.resolved_errors,
                    'recurring_errors': stats.recurring_errors,
                    'system_impact_score': stats.system_impact_score,
                    'avg_resolution_time': stats.avg_resolution_time
                },
                'error_breakdown': {
                    'by_type': {k.value: v for k, v in stats.errors_by_type.items()},
                    'by_severity': {k.value: v for k, v in stats.errors_by_severity.items()},
                    'by_module': stats.errors_by_module,
                    'by_hour': stats.errors_by_hour
                },
                'top_errors': sorted(
                    [(k, v) for k, v in stats.error_frequency.items()],
                    key=lambda x: x[1], reverse=True
                )[:20],
                'system_health': health_status,
                'recent_errors': error_logs[:50]  # æœ€è¿‘50ä¸ªé”™è¯¯
            }
            
            # å†™å…¥æ–‡ä»¶
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2, default=str)
            
            logging.info(f"é”™è¯¯æŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {output_file}")
            return True
            
        except Exception as e:
            logging.error(f"å¯¼å‡ºé”™è¯¯æŠ¥å‘Šå¤±è´¥: {str(e)}")
            return False
    
    def shutdown(self):
        """å…³é—­é”™è¯¯æ—¥å¿—è®°å½•å™¨"""
        try:
            # åœæ­¢å¼‚æ­¥å¤„ç†ä»»åŠ¡
            for task in self.processing_tasks:
                task.cancel()
            
            # ç­‰å¾…ä»»åŠ¡å®Œæˆ
            if self.processing_tasks:
                loop = asyncio.get_event_loop()
                loop.run_until_complete(
                    asyncio.gather(*self.processing_tasks, return_exceptions=True)
                )
            
            # æ¸…ç†èµ„æº
            self.processing_tasks.clear()
            
            logging.info("é”™è¯¯æ—¥å¿—è®°å½•å™¨å·²å…³é—­")
            
        except Exception as e:
            logging.error(f"å…³é—­é”™è¯¯æ—¥å¿—è®°å½•å™¨æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")


def create_sample_config() -> Dict[str, Any]:
    """
    åˆ›å»ºç¤ºä¾‹é…ç½®
    
    Returns:
        ç¤ºä¾‹é…ç½®å­—å…¸
    """
    return {
        'database_path': 'error_logs.db',
        'enable_alerts': True,
        'enable_recovery': True,
        'enable_statistics': True,
        'async_processing': True,
        'alert_rules': {
            'system_critical': {'enabled': True},
            'business_high': {'enabled': True},
            'network_medium': {'enabled': False},
            'unknown_low': {'enabled': False}
        },
        'notification_channels': {
            'email': {
                'smtp_server': 'smtp.example.com',
                'smtp_port': 587,
                'use_tls': True,
                'from': 'alerts@example.com',
                'to': ['admin@example.com'],
                'username': 'alerts@example.com',
                'password': 'your_password'
            },
            'webhook': {
                'url': 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK',
                'headers': {'Content-Type': 'application/json'}
            },
            'slack': {
                'webhook_url': 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
            },
            'dingtalk': {
                'webhook_url': 'https://oapi.dingtalk.com/robot/send?access_token=YOUR_TOKEN'
            }
        },
        'recovery_strategies': {
            'network_high': 'retry',
            'system_critical': 'fallback',
            'business_medium': 'ignore',
            'database_high': 'circuit_breaker'
        },
        'retry_policies': {
            'default': {
                'max_retries': 3,
                'retry_delay': 1.0,
                'backoff_factor': 2.0
            }
        },
        'default_timeout': 30.0
    }


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # åˆ›å»ºé…ç½®
    config = create_sample_config()
    
    # åˆå§‹åŒ–é”™è¯¯æ—¥å¿—è®°å½•å™¨
    error_logger = ErrorLogger(config)
    
    # ç¤ºä¾‹1: ä½¿ç”¨è£…é¥°å™¨
    @error_logger.error_handler(
        error_type=ErrorType.BUSINESS,
        severity=ErrorSeverity.HIGH,
        recovery_func=lambda: "fallback_result"
    )
    def risky_business_function():
        """æ¨¡æ‹Ÿä¸šåŠ¡å‡½æ•°"""
        import random
        if random.random() < 0.7:  # 70%æ¦‚ç‡å‡ºé”™
            raise ValueError("ä¸šåŠ¡é€»è¾‘éªŒè¯å¤±è´¥")
        return "ä¸šåŠ¡å¤„ç†æˆåŠŸ"
    
    # ç¤ºä¾‹2: æ‰‹åŠ¨è®°å½•é”™è¯¯
    try:
        # æ¨¡æ‹Ÿç³»ç»Ÿé”™è¯¯
        result = 1 / 0
    except ZeroDivisionError as e:
        error_id = error_logger.log_error(
            error_type=ErrorType.SYSTEM,
            severity=ErrorSeverity.CRITICAL,
            message="é™¤é›¶é”™è¯¯",
            exception=e,
            additional_data={'operation': 'division', 'operand': 0},
            service_name="example_service"
        )
        print(f"è®°å½•ç³»ç»Ÿé”™è¯¯: {error_id}")
    
    # ç¤ºä¾‹3: ç½‘ç»œé”™è¯¯
    try:
        import requests
        response = requests.get("https://nonexistent-domain-12345.com", timeout=1)
    except Exception as e:
        error_id = error_logger.log_error(
            error_type=ErrorType.NETWORK,
            severity=ErrorSeverity.MEDIUM,
            message="ç½‘ç»œè¯·æ±‚å¤±è´¥",
            exception=e,
            additional_data={'url': 'https://nonexistent-domain-12345.com'}
        )
        print(f"è®°å½•ç½‘ç»œé”™è¯¯: {error_id}")
    
    # ç¤ºä¾‹4: æµ‹è¯•æ¢å¤æœºåˆ¶
    def recovery_function():
        """æ¢å¤å‡½æ•°"""
        return "æ¢å¤æˆåŠŸ"
    
    success, result = error_logger.attempt_recovery(
        error_type=ErrorType.BUSINESS,
        recovery_func=recovery_function
    )
    print(f"æ¢å¤ç»“æœ: {success}, {result}")
    
    # ç¤ºä¾‹5: æ‰§è¡Œå¯èƒ½å‡ºé”™çš„å‡½æ•°
    print("\n=== æµ‹è¯•è£…é¥°å™¨åŠŸèƒ½ ===")
    for i in range(5):
        try:
            result = risky_business_function()
            print(f"è°ƒç”¨ {i+1}: {result}")
        except Exception as e:
            print(f"è°ƒç”¨ {i+1}: å¼‚å¸¸è¢«è£…é¥°å™¨æ•è·å¹¶å¤„ç†")
    
    # ç¤ºä¾‹6: è·å–ç»Ÿè®¡ä¿¡æ¯
    print("\n=== ç»Ÿè®¡ä¿¡æ¯ ===")
    stats = error_logger.get_statistics(hours=1)
    print(f"æ€»é”™è¯¯æ•°: {stats.total_errors}")
    print(f"ä¸¥é‡é”™è¯¯æ•°: {stats.errors_by_severity.get(ErrorSeverity.CRITICAL, 0)}")
    print(f"ç³»ç»Ÿå½±å“åˆ†æ•°: {stats.system_impact_score}")
    
    # ç¤ºä¾‹7: è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€
    print("\n=== ç³»ç»Ÿå¥åº·çŠ¶æ€ ===")
    health = error_logger.get_system_health()
    print(f"å¥åº·çŠ¶æ€: {health['status']}")
    print(f"å¥åº·åˆ†æ•°: {health['health_score']}")
    print(f"é”™è¯¯ç‡(æ¯å°æ—¶): {health['error_rate_per_hour']}")
    
    # ç¤ºä¾‹8: è·å–æœ€è¿‘çš„é”™è¯¯
    print("\n=== æœ€è¿‘çš„é”™è¯¯ ===")
    recent_errors = error_logger.get_recent_errors(5)
    for error in recent_errors:
        print(f"é”™è¯¯ID: {error.error_id}")
        print(f"ç±»å‹: {error.error_type.value}")
        print(f"ä¸¥é‡ç¨‹åº¦: {error.severity.value}")
        print(f"æ¶ˆæ¯: {error.message}")
        print(f"æ—¶é—´: {error.timestamp}")
        print("---")
    
    # ç¤ºä¾‹9: å¯¼å‡ºé”™è¯¯æŠ¥å‘Š
    print("\n=== å¯¼å‡ºé”™è¯¯æŠ¥å‘Š ===")
    success = error_logger.export_error_report("error_report.json", hours=1)
    if success:
        print("é”™è¯¯æŠ¥å‘Šå·²å¯¼å‡ºåˆ° error_report.json")
    
    # ç¤ºä¾‹10: è§£å†³å’Œå…³é—­é”™è¯¯
    if recent_errors:
        error_id = recent_errors[0].error_id
        success = error_logger.resolve_error(error_id, "é—®é¢˜å·²ä¿®å¤")
        print(f"è§£å†³é”™è¯¯ {error_id}: {success}")
        
        success = error_logger.close_error(error_id, "éªŒè¯é€šè¿‡ï¼Œå…³é—­é—®é¢˜")
        print(f"å…³é—­é”™è¯¯ {error_id}: {success}")
    
    # ç­‰å¾…å¼‚æ­¥å¤„ç†å®Œæˆ
    time.sleep(2)
    
    # å…³é—­é”™è¯¯æ—¥å¿—è®°å½•å™¨
    error_logger.shutdown()
    
    print("\n=== ç¤ºä¾‹å®Œæˆ ===")


class ErrorTrendAnalyzer:
    """é”™è¯¯è¶‹åŠ¿åˆ†æå™¨"""
    
    def __init__(self, error_logger: ErrorLogger):
        self.error_logger = error_logger
        self.trend_cache = {}
        self._lock = threading.RLock()
    
    def analyze_error_trends(self, days: int = 7) -> Dict[str, Any]:
        """åˆ†æé”™è¯¯è¶‹åŠ¿"""
        try:
            trends = {
                'period_days': days,
                'daily_error_counts': {},
                'error_growth_rate': 0.0,
                'peak_error_day': None,
                'error_patterns': {},
                'seasonal_analysis': {},
                'correlation_analysis': {},
                'prediction': {}
            }
            
            # è·å–æ¯æ—¥é”™è¯¯ç»Ÿè®¡
            for day_offset in range(days):
                date = datetime.now() - timedelta(days=day_offset)
                date_str = date.strftime('%Y-%m-%d')
                
                start_time = date.replace(hour=0, minute=0, second=0, microsecond=0)
                end_time = date.replace(hour=23, minute=59, second=59, microsecond=999999)
                
                day_errors = self.error_logger.get_error_logs(
                    start_date=start_time,
                    end_date=end_time,
                    limit=10000
                )
                
                trends['daily_error_counts'][date_str] = len(day_errors)
                
                # æŒ‰å°æ—¶ç»Ÿè®¡
                hourly_counts = {}
                for error in day_errors:
                    error_time = datetime.fromisoformat(error['timestamp'])
                    hour = error_time.hour
                    hourly_counts[hour] = hourly_counts.get(hour, 0) + 1
                
                trends['error_patterns'][date_str] = hourly_counts
            
            # è®¡ç®—å¢é•¿ç‡
            if len(trends['daily_error_counts']) >= 2:
                counts = list(trends['daily_error_counts'].values())
                if counts[-1] > 0:
                    growth_rate = (counts[0] - counts[-1]) / counts[-1] * 100
                    trends['error_growth_rate'] = growth_rate
            
            # æ‰¾å‡ºå³°å€¼é”™è¯¯æ—¥
            if trends['daily_error_counts']:
                peak_day = max(trends['daily_error_counts'], key=trends['daily_error_counts'].get)
                trends['peak_error_day'] = {
                    'date': peak_day,
                    'error_count': trends['daily_error_counts'][peak_day]
                }
            
            # å­£èŠ‚æ€§åˆ†æ
            trends['seasonal_analysis'] = self._analyze_seasonal_patterns(trends['daily_error_counts'])
            
            # ç›¸å…³æ€§åˆ†æ
            trends['correlation_analysis'] = self._analyze_correlations()
            
            # é¢„æµ‹åˆ†æ
            trends['prediction'] = self._predict_future_errors(trends['daily_error_counts'])
            
            return trends
            
        except Exception as e:
            logging.error(f"é”™è¯¯è¶‹åŠ¿åˆ†æå¤±è´¥: {str(e)}")
            return {}
    
    def _analyze_seasonal_patterns(self, daily_counts: Dict[str, int]) -> Dict[str, Any]:
        """åˆ†æå­£èŠ‚æ€§æ¨¡å¼"""
        try:
            patterns = {
                'weekday_vs_weekend': {},
                'hourly_distribution': {},
                'monthly_trend': {}
            }
            
            weekday_errors = 0
            weekend_errors = 0
            hourly_totals = defaultdict(int)
            
            for date_str, count in daily_counts.items():
                date_obj = datetime.strptime(date_str, '%Y-%m-%d')
                weekday = date_obj.weekday()
                
                if weekday < 5:  # å‘¨ä¸€åˆ°å‘¨äº”
                    weekday_errors += count
                else:  # å‘¨å…­å’Œå‘¨æ—¥
                    weekend_errors += count
            
            patterns['weekday_vs_weekend'] = {
                'weekday_avg': weekday_errors / max(len(daily_counts) * 5 / 7, 1),
                'weekend_avg': weekend_errors / max(len(daily_counts) * 2 / 7, 1),
                'weekday_total': weekday_errors,
                'weekend_total': weekend_errors
            }
            
            # è·å–æœ€è¿‘çš„æ•°æ®è¿›è¡Œå°æ—¶åˆ†æ
            recent_logs = self.error_logger.get_error_logs(limit=1000)
            for log in recent_logs:
                error_time = datetime.fromisoformat(log['timestamp'])
                hour = error_time.hour
                hourly_totals[hour] += 1
            
            patterns['hourly_distribution'] = dict(hourly_totals)
            
            return patterns
            
        except Exception as e:
            logging.error(f"å­£èŠ‚æ€§æ¨¡å¼åˆ†æå¤±è´¥: {str(e)}")
            return {}
    
    def _analyze_correlations(self) -> Dict[str, Any]:
        """åˆ†æç›¸å…³æ€§"""
        try:
            correlations = {
                'error_type_correlation': {},
                'severity_correlation': {},
                'module_correlation': {},
                'time_correlation': {}
            }
            
            # è·å–æœ€è¿‘çš„é”™è¯¯æ—¥å¿—
            logs = self.error_logger.get_error_logs(limit=1000)
            
            # é”™è¯¯ç±»å‹ç›¸å…³æ€§
            type_counts = defaultdict(int)
            for log in logs:
                type_counts[log['error_type']] += 1
            
            correlations['error_type_correlation'] = dict(type_counts)
            
            # ä¸¥é‡ç¨‹åº¦ç›¸å…³æ€§
            severity_counts = defaultdict(int)
            for log in logs:
                severity_counts[log['severity']] += 1
            
            correlations['severity_correlation'] = dict(severity_counts)
            
            # æ¨¡å—ç›¸å…³æ€§
            module_counts = defaultdict(int)
            for log in logs:
                module_counts[log['module_name']] += 1
            
            correlations['module_correlation'] = dict(module_counts)
            
            return correlations
            
        except Exception as e:
            logging.error(f"ç›¸å…³æ€§åˆ†æå¤±è´¥: {str(e)}")
            return {}
    
    def _predict_future_errors(self, daily_counts: Dict[str, int]) -> Dict[str, Any]:
        """é¢„æµ‹æœªæ¥é”™è¯¯"""
        try:
            prediction = {
                'method': 'linear_regression',
                'next_7_days': [],
                'confidence': 0.0,
                'factors': []
            }
            
            if len(daily_counts) < 3:
                return prediction
            
            # ç®€å•çš„çº¿æ€§å›å½’é¢„æµ‹
            counts = list(daily_counts.values())
            days = list(range(len(counts)))
            
            # è®¡ç®—è¶‹åŠ¿
            if len(counts) >= 2:
                trend = (counts[-1] - counts[0]) / len(counts)
                last_value = counts[-1]
                
                # é¢„æµ‹æœªæ¥7å¤©
                for i in range(1, 8):
                    predicted_value = max(0, last_value + trend * i)
                    prediction['next_7_days'].append(round(predicted_value))
                
                # è®¡ç®—ç½®ä¿¡åº¦
                if len(counts) >= 5:
                    variance = sum((x - sum(counts)/len(counts))**2 for x in counts) / len(counts)
                    prediction['confidence'] = max(0, 1 - variance / 100)
            
            return prediction
            
        except Exception as e:
            logging.error(f"é”™è¯¯é¢„æµ‹å¤±è´¥: {str(e)}")
            return {}


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self, error_logger: ErrorLogger):
        self.error_logger = error_logger
        self.performance_data = deque(maxlen=10000)
        self.thresholds = {
            'response_time': 5.0,  # ç§’
            'memory_usage': 1000,  # MB
            'cpu_usage': 80.0,     # ç™¾åˆ†æ¯”
            'error_rate': 10.0     # æ¯å°æ—¶é”™è¯¯æ•°
        }
        self._lock = threading.RLock()
    
    def record_performance(self, 
                          operation: str,
                          duration: float,
                          memory_usage: float = None,
                          cpu_usage: float = None,
                          success: bool = True,
                          additional_data: Dict[str, Any] = None) -> bool:
        """è®°å½•æ€§èƒ½æ•°æ®"""
        try:
            with self._lock:
                performance_record = {
                    'timestamp': time.time(),
                    'operation': operation,
                    'duration': duration,
                    'memory_usage': memory_usage or psutil.Process().memory_info().rss / 1024 / 1024,
                    'cpu_usage': cpu_usage or psutil.Process().cpu_percent(),
                    'success': success,
                    'additional_data': additional_data or {}
                }
                
                self.performance_data.append(performance_record)
                
                # æ£€æŸ¥é˜ˆå€¼
                self._check_performance_thresholds(performance_record)
                
                return True
                
        except Exception as e:
            logging.error(f"è®°å½•æ€§èƒ½æ•°æ®å¤±è´¥: {str(e)}")
            return False
    
    def _check_performance_thresholds(self, record: Dict[str, Any]):
        """æ£€æŸ¥æ€§èƒ½é˜ˆå€¼"""
        try:
            violations = []
            
            if record['duration'] > self.thresholds['response_time']:
                violations.append(f"å“åº”æ—¶é—´è¶…é™: {record['duration']:.2f}s > {self.thresholds['response_time']}s")
            
            if record['memory_usage'] > self.thresholds['memory_usage']:
                violations.append(f"å†…å­˜ä½¿ç”¨è¶…é™: {record['memory_usage']:.1f}MB > {self.thresholds['memory_usage']}MB")
            
            if record['cpu_usage'] > self.thresholds['cpu_usage']:
                violations.append(f"CPUä½¿ç”¨ç‡è¶…é™: {record['cpu_usage']:.1f}% > {self.thresholds['cpu_usage']}%")
            
            if violations:
                # è®°å½•æ€§èƒ½è¿è§„
                message = f"æ€§èƒ½ç›‘æ§è¿è§„: {'; '.join(violations)}"
                self.error_logger.log_error(
                    error_type=ErrorType.PERFORMANCE,
                    severity=ErrorSeverity.MEDIUM,
                    message=message,
                    additional_data=record
                )
                
        except Exception as e:
            logging.error(f"æ£€æŸ¥æ€§èƒ½é˜ˆå€¼å¤±è´¥: {str(e)}")
    
    def get_performance_summary(self, hours: int = 1) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        try:
            cutoff_time = time.time() - (hours * 3600)
            
            with self._lock:
                recent_records = [
                    record for record in self.performance_data 
                    if record['timestamp'] >= cutoff_time
                ]
            
            if not recent_records:
                return {'message': 'æ²¡æœ‰æ€§èƒ½æ•°æ®'}
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            durations = [r['duration'] for r in recent_records]
            memory_usage = [r['memory_usage'] for r in recent_records]
            cpu_usage = [r['cpu_usage'] for r in recent_records]
            success_rate = sum(1 for r in recent_records if r['success']) / len(recent_records) * 100
            
            summary = {
                'period_hours': hours,
                'total_operations': len(recent_records),
                'success_rate': round(success_rate, 2),
                'response_time': {
                    'avg': round(sum(durations) / len(durations), 3),
                    'min': round(min(durations), 3),
                    'max': round(max(durations), 3),
                    'p95': round(sorted(durations)[int(len(durations) * 0.95)], 3) if durations else 0
                },
                'memory_usage': {
                    'avg': round(sum(memory_usage) / len(memory_usage), 1),
                    'min': round(min(memory_usage), 1),
                    'max': round(max(memory_usage), 1)
                },
                'cpu_usage': {
                    'avg': round(sum(cpu_usage) / len(cpu_usage), 1),
                    'min': round(min(cpu_usage), 1),
                    'max': round(max(cpu_usage), 1)
                },
                'operations_by_type': {}
            }
            
            # æŒ‰æ“ä½œç±»å‹ç»Ÿè®¡
            for record in recent_records:
                op_type = record['operation']
                if op_type not in summary['operations_by_type']:
                    summary['operations_by_type'][op_type] = {
                        'count': 0,
                        'success_count': 0,
                        'avg_duration': 0,
                        'total_duration': 0
                    }
                
                op_stats = summary['operations_by_type'][op_type]
                op_stats['count'] += 1
                op_stats['total_duration'] += record['duration']
                if record['success']:
                    op_stats['success_count'] += 1
            
            # è®¡ç®—å¹³å‡å€¼
            for op_type, stats in summary['operations_by_type'].items():
                if stats['count'] > 0:
                    stats['success_rate'] = round(stats['success_count'] / stats['count'] * 100, 2)
                    stats['avg_duration'] = round(stats['total_duration'] / stats['count'], 3)
            
            return summary
            
        except Exception as e:
            logging.error(f"è·å–æ€§èƒ½æ‘˜è¦å¤±è´¥: {str(e)}")
            return {'error': str(e)}
    
    def set_thresholds(self, **kwargs):
        """è®¾ç½®æ€§èƒ½é˜ˆå€¼"""
        try:
            with self._lock:
                for key, value in kwargs.items():
                    if key in self.thresholds:
                        self.thresholds[key] = value
                        logging.info(f"æ€§èƒ½é˜ˆå€¼å·²æ›´æ–°: {key} = {value}")
        except Exception as e:
            logging.error(f"è®¾ç½®æ€§èƒ½é˜ˆå€¼å¤±è´¥: {str(e)}")


class ConfigurationManager:
    """é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_file: str = "error_logger_config.json"):
        self.config_file = config_file
        self.config = self._load_config()
        self._observers = []
        self._lock = threading.RLock()
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            if os.path.exists(self.config_file):
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            else:
                # åˆ›å»ºé»˜è®¤é…ç½®
                default_config = create_sample_config()
                self._save_config(default_config)
                return default_config
        except Exception as e:
            logging.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
            return create_sample_config()
    
    def _save_config(self, config: Dict[str, Any]):
        """ä¿å­˜é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logging.error(f"ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def get(self, key: str, default=None):
        """è·å–é…ç½®å€¼"""
        try:
            with self._lock:
                keys = key.split('.')
                value = self.config
                for k in keys:
                    if isinstance(value, dict) and k in value:
                        value = value[k]
                    else:
                        return default
                return value
        except Exception:
            return default
    
    def set(self, key: str, value: Any):
        """è®¾ç½®é…ç½®å€¼"""
        try:
            with self._lock:
                keys = key.split('.')
                config = self.config
                for k in keys[:-1]:
                    if k not in config:
                        config[k] = {}
                    config = config[k]
                config[keys[-1]] = value
                
                # ä¿å­˜é…ç½®
                self._save_config(self.config)
                
                # é€šçŸ¥è§‚å¯Ÿè€…
                self._notify_observers(key, value)
                
        except Exception as e:
            logging.error(f"è®¾ç½®é…ç½®å€¼å¤±è´¥: {str(e)}")
    
    def add_observer(self, callback: Callable[[str, Any], None]):
        """æ·»åŠ é…ç½®è§‚å¯Ÿè€…"""
        try:
            with self._lock:
                self._observers.append(callback)
        except Exception as e:
            logging.error(f"æ·»åŠ é…ç½®è§‚å¯Ÿè€…å¤±è´¥: {str(e)}")
    
    def _notify_observers(self, key: str, value: Any):
        """é€šçŸ¥è§‚å¯Ÿè€…"""
        try:
            for callback in self._observers:
                try:
                    callback(key, value)
                except Exception as e:
                    logging.error(f"é…ç½®è§‚å¯Ÿè€…å›è°ƒå¤±è´¥: {str(e)}")
        except Exception as e:
            logging.error(f"é€šçŸ¥è§‚å¯Ÿè€…å¤±è´¥: {str(e)}")
    
    def validate_config(self) -> Tuple[bool, List[str]]:
        """éªŒè¯é…ç½®"""
        try:
            errors = []
            
            # æ£€æŸ¥å¿…éœ€çš„é…ç½®é¡¹
            required_keys = [
                'database_path',
                'enable_alerts',
                'enable_recovery',
                'enable_statistics'
            ]
            
            for key in required_keys:
                if self.get(key) is None:
                    errors.append(f"ç¼ºå°‘å¿…éœ€é…ç½®é¡¹: {key}")
            
            # æ£€æŸ¥æ•°æ®åº“è·¯å¾„
            db_path = self.get('database_path')
            if db_path:
                db_dir = os.path.dirname(db_path)
                if db_dir and not os.path.exists(db_dir):
                    try:
                        os.makedirs(db_dir, exist_ok=True)
                    except Exception as e:
                        errors.append(f"æ— æ³•åˆ›å»ºæ•°æ®åº“ç›®å½•: {db_dir}, é”™è¯¯: {str(e)}")
            
            # æ£€æŸ¥å‘Šè­¦é…ç½®
            if self.get('enable_alerts'):
                email_config = self.get('notification_channels.email')
                if email_config:
                    required_email_keys = ['smtp_server', 'from', 'to']
                    for key in required_email_keys:
                        if not email_config.get(key):
                            errors.append(f"é‚®ä»¶é…ç½®ç¼ºå°‘: {key}")
            
            return len(errors) == 0, errors
            
        except Exception as e:
            return False, [f"é…ç½®éªŒè¯å¼‚å¸¸: {str(e)}"]
    
    def reset_to_default(self):
        """é‡ç½®ä¸ºé»˜è®¤é…ç½®"""
        try:
            with self._lock:
                self.config = create_sample_config()
                self._save_config(self.config)
                logging.info("é…ç½®å·²é‡ç½®ä¸ºé»˜è®¤å€¼")
        except Exception as e:
            logging.error(f"é‡ç½®é…ç½®å¤±è´¥: {str(e)}")


class ErrorPatternRecognizer:
    """é”™è¯¯æ¨¡å¼è¯†åˆ«å™¨"""
    
    def __init__(self, error_logger: ErrorLogger):
        self.error_logger = error_logger
        self.patterns = {}
        self._lock = threading.RLock()
    
    def learn_patterns(self, days: int = 7) -> Dict[str, Any]:
        """å­¦ä¹ é”™è¯¯æ¨¡å¼"""
        try:
            patterns = {
                'recurring_errors': {},
                'error_clusters': {},
                'time_patterns': {},
                'sequence_patterns': {},
                'anomaly_detection': {}
            }
            
            # è·å–å†å²é”™è¯¯æ•°æ®
            end_time = datetime.now()
            start_time = end_time - timedelta(days=days)
            
            error_logs = self.error_logger.get_error_logs(
                start_date=start_time,
                end_date=end_time,
                limit=10000
            )
            
            # è¯†åˆ«é‡å¤é”™è¯¯
            patterns['recurring_errors'] = self._identify_recurring_errors(error_logs)
            
            # é”™è¯¯èšç±»
            patterns['error_clusters'] = self._cluster_errors(error_logs)
            
            # æ—¶é—´æ¨¡å¼
            patterns['time_patterns'] = self._analyze_time_patterns(error_logs)
            
            # åºåˆ—æ¨¡å¼
            patterns['sequence_patterns'] = self._analyze_sequence_patterns(error_logs)
            
            # å¼‚å¸¸æ£€æµ‹
            patterns['anomaly_detection'] = self._detect_anomalies(error_logs)
            
            with self._lock:
                self.patterns = patterns
            
            return patterns
            
        except Exception as e:
            logging.error(f"å­¦ä¹ é”™è¯¯æ¨¡å¼å¤±è´¥: {str(e)}")
            return {}
    
    def _identify_recurring_errors(self, error_logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è¯†åˆ«é‡å¤é”™è¯¯"""
        try:
            recurring = {}
            error_signatures = defaultdict(list)
            
            for log in error_logs:
                # åˆ›å»ºé”™è¯¯ç­¾å
                signature = f"{log['error_type']}_{log['module_name']}_{log['function_name']}"
                error_signatures[signature].append(log)
            
            # æ‰¾å‡ºé‡å¤æ¬¡æ•°è¶…è¿‡é˜ˆå€¼çš„é”™è¯¯
            threshold = 3
            for signature, logs in error_signatures.items():
                if len(logs) >= threshold:
                    recurring[signature] = {
                        'count': len(logs),
                        'first_occurrence': min(log['timestamp'] for log in logs),
                        'last_occurrence': max(log['timestamp'] for log in logs),
                        'error_type': logs[0]['error_type'],
                        'module': logs[0]['module_name'],
                        'function': logs[0]['function_name']
                    }
            
            return recurring
            
        except Exception as e:
            logging.error(f"è¯†åˆ«é‡å¤é”™è¯¯å¤±è´¥: {str(e)}")
            return {}
    
    def _cluster_errors(self, error_logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """é”™è¯¯èšç±»"""
        try:
            clusters = {
                'by_type': defaultdict(list),
                'by_severity': defaultdict(list),
                'by_module': defaultdict(list),
                'by_time_window': defaultdict(list)
            }
            
            for log in error_logs:
                # æŒ‰ç±»å‹èšç±»
                clusters['by_type'][log['error_type']].append(log)
                
                # æŒ‰ä¸¥é‡ç¨‹åº¦èšç±»
                clusters['by_severity'][log['severity']].append(log)
                
                # æŒ‰æ¨¡å—èšç±»
                clusters['by_module'][log['module_name']].append(log)
                
                # æŒ‰æ—¶é—´çª—å£èšç±»ï¼ˆæ¯å°æ—¶ï¼‰
                error_time = datetime.fromisoformat(log['timestamp'])
                time_window = error_time.strftime('%Y-%m-%d %H:00')
                clusters['by_time_window'][time_window].append(log)
            
            # è½¬æ¢ä¸ºæ™®é€šå­—å…¸
            result = {}
            for cluster_type, cluster_data in clusters.items():
                result[cluster_type] = {k: len(v) for k, v in cluster_data.items()}
            
            return result
            
        except Exception as e:
            logging.error(f"é”™è¯¯èšç±»å¤±è´¥: {str(e)}")
            return {}
    
    def _analyze_time_patterns(self, error_logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ææ—¶é—´æ¨¡å¼"""
        try:
            patterns = {
                'hourly_distribution': defaultdict(int),
                'daily_distribution': defaultdict(int),
                'weekly_distribution': defaultdict(int),
                'peak_hours': [],
                'quiet_hours': []
            }
            
            for log in error_logs:
                error_time = datetime.fromisoformat(log['timestamp'])
                
                patterns['hourly_distribution'][error_time.hour] += 1
                patterns['daily_distribution'][error_time.weekday()] += 1
                patterns['weekly_distribution'][error_time.strftime('%A')] += 1
            
            # æ‰¾å‡ºé«˜å³°å’Œä½è°·æ—¶é—´
            hourly_counts = patterns['hourly_distribution']
            if hourly_counts:
                sorted_hours = sorted(hourly_counts.items(), key=lambda x: x[1], reverse=True)
                patterns['peak_hours'] = [hour for hour, count in sorted_hours[:3]]
                patterns['quiet_hours'] = [hour for hour, count in sorted_hours[-3:]]
            
            return dict(patterns)
            
        except Exception as e:
            logging.error(f"æ—¶é—´æ¨¡å¼åˆ†æå¤±è´¥: {str(e)}")
            return {}
    
    def _analyze_sequence_patterns(self, error_logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æåºåˆ—æ¨¡å¼"""
        try:
            sequences = {}
            
            # æŒ‰æ—¶é—´æ’åº
            sorted_logs = sorted(error_logs, key=lambda x: x['timestamp'])
            
            # æŸ¥æ‰¾è¿ç»­çš„é”™è¯¯åºåˆ—
            current_sequence = []
            sequence_threshold = 300  # 5åˆ†é’Ÿå†…è®¤ä¸ºæ˜¯è¿ç»­åºåˆ—
            
            for i, log in enumerate(sorted_logs):
                if not current_sequence:
                    current_sequence.append(log)
                else:
                    prev_time = datetime.fromisoformat(current_sequence[-1]['timestamp'])
                    curr_time = datetime.fromisoformat(log['timestamp'])
                    
                    if (curr_time - prev_time).total_seconds() <= sequence_threshold:
                        current_sequence.append(log)
                    else:
                        if len(current_sequence) >= 2:
                            seq_key = f"{current_sequence[0]['error_type']}->{current_sequence[-1]['error_type']}"
                            if seq_key not in sequences:
                                sequences[seq_key] = []
                            sequences[seq_key].append({
                                'length': len(current_sequence),
                                'duration': (curr_time - prev_time).total_seconds(),
                                'errors': [log['error_type'] for log in current_sequence]
                            })
                        current_sequence = [log]
            
            return sequences
            
        except Exception as e:
            logging.error(f"åºåˆ—æ¨¡å¼åˆ†æå¤±è´¥: {str(e)}")
            return {}
    
    def _detect_anomalies(self, error_logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ£€æµ‹å¼‚å¸¸"""
        try:
            anomalies = {
                'sudden_spikes': [],
                'unusual_patterns': [],
                'statistical_outliers': []
            }
            
            # æŒ‰å°æ—¶ç»Ÿè®¡é”™è¯¯æ•°é‡
            hourly_counts = defaultdict(int)
            for log in error_logs:
                error_time = datetime.fromisoformat(log['timestamp'])
                hour_key = error_time.strftime('%Y-%m-%d %H')
                hourly_counts[hour_key] += 1
            
            if hourly_counts:
                counts = list(hourly_counts.values())
                mean_count = sum(counts) / len(counts)
                std_count = (sum((x - mean_count)**2 for x in counts) / len(counts)) ** 0.5
                
                # æ£€æµ‹çªç„¶å³°å€¼
                for hour, count in hourly_counts.items():
                    if count > mean_count + 2 * std_count:
                        anomalies['sudden_spikes'].append({
                            'hour': hour,
                            'error_count': count,
                            'expected_count': round(mean_count, 1),
                            'deviation': round((count - mean_count) / std_count, 2)
                        })
            
            return anomalies
            
        except Exception as e:
            logging.error(f"å¼‚å¸¸æ£€æµ‹å¤±è´¥: {str(e)}")
            return {}
    
    def predict_next_errors(self) -> List[Dict[str, Any]]:
        """é¢„æµ‹ä¸‹ä¸€ä¸ªå¯èƒ½å‘ç”Ÿçš„é”™è¯¯"""
        try:
            predictions = []
            
            with self._lock:
                if not self.patterns:
                    return predictions
                
                # åŸºäºé‡å¤é”™è¯¯é¢„æµ‹
                recurring = self.patterns.get('recurring_errors', {})
                for signature, info in recurring.items():
                    predictions.append({
                        'type': 'recurring_error',
                        'signature': signature,
                        'probability': min(info['count'] / 10.0, 1.0),  # ç®€å•æ¦‚ç‡è®¡ç®—
                        'description': f"é”™è¯¯ {signature} å¯èƒ½ä¼šå†æ¬¡å‘ç”Ÿ",
                        'recommendation': "æ£€æŸ¥ç›¸å…³ä»£ç å’Œé…ç½®"
                    })
                
                # åŸºäºæ—¶é—´æ¨¡å¼é¢„æµ‹
                time_patterns = self.patterns.get('time_patterns', {})
                peak_hours = time_patterns.get('peak_hours', [])
                if peak_hours:
                    current_hour = datetime.now().hour
                    if current_hour in peak_hours:
                        predictions.append({
                            'type': 'time_based',
                            'hour': current_hour,
                            'probability': 0.7,
                            'description': f"å½“å‰æ—¶é—´ {current_hour}:00 æ˜¯é”™è¯¯é«˜å³°æœŸ",
                            'recommendation': "åŠ å¼ºç›‘æ§å’Œé¢„è­¦"
                        })
            
            return predictions
            
        except Exception as e:
            logging.error(f"é¢„æµ‹ä¸‹ä¸€ä¸ªé”™è¯¯å¤±è´¥: {str(e)}")
            return []


class AutomatedTestSuite:
    """è‡ªåŠ¨åŒ–æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self, error_logger: ErrorLogger):
        self.error_logger = error_logger
        self.test_results = []
        self._lock = threading.RLock()
    
    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        try:
            test_results = {
                'total_tests': 0,
                'passed_tests': 0,
                'failed_tests': 0,
                'test_details': [],
                'overall_status': 'unknown'
            }
            
            # æµ‹è¯•ç”¨ä¾‹
            test_cases = [
                self.test_basic_logging,
                self.test_error_classification,
                self.test_alert_system,
                self.test_recovery_mechanism,
                self.test_statistics_generation,
                self.test_database_operations,
                self.test_performance_monitoring,
                self.test_configuration_management,
                self.test_pattern_recognition,
                self.test_async_processing
            ]
            
            for test_case in test_cases:
                try:
                    test_results['total_tests'] += 1
                    result = test_case()
                    test_results['test_details'].append(result)
                    
                    if result['status'] == 'passed':
                        test_results['passed_tests'] += 1
                    else:
                        test_results['failed_tests'] += 1
                        
                except Exception as e:
                    test_results['total_tests'] += 1
                    test_results['failed_tests'] += 1
                    test_results['test_details'].append({
                        'test_name': test_case.__name__,
                        'status': 'failed',
                        'error': str(e)
                    })
            
            # ç¡®å®šæ•´ä½“çŠ¶æ€
            if test_results['failed_tests'] == 0:
                test_results['overall_status'] = 'passed'
            elif test_results['passed_tests'] == test_results['total_tests'] // 2:
                test_results['overall_status'] = 'partial'
            else:
                test_results['overall_status'] = 'failed'
            
            with self._lock:
                self.test_results.append(test_results)
            
            return test_results
            
        except Exception as e:
            logging.error(f"è¿è¡Œæµ‹è¯•å¥—ä»¶å¤±è´¥: {str(e)}")
            return {'error': str(e)}
    
    def test_basic_logging(self) -> Dict[str, Any]:
        """æµ‹è¯•åŸºæœ¬æ—¥å¿—è®°å½•åŠŸèƒ½"""
        try:
            # æµ‹è¯•è®°å½•ä¸åŒç±»å‹çš„é”™è¯¯
            error_types = [ErrorType.SYSTEM, ErrorType.BUSINESS, ErrorType.NETWORK]
            error_ids = []
            
            for error_type in error_types:
                error_id = self.error_logger.log_error(
                    error_type=error_type,
                    severity=ErrorSeverity.MEDIUM,
                    message=f"æµ‹è¯•é”™è¯¯ - {error_type.value}",
                    additional_data={'test': True}
                )
                error_ids.append(error_id)
            
            # éªŒè¯é”™è¯¯æ˜¯å¦è¢«è®°å½•
            recent_errors = self.error_logger.get_recent_errors(len(error_ids))
            
            if len(recent_errors) >= len(error_ids):
                return {
                    'test_name': 'test_basic_logging',
                    'status': 'passed',
                    'message': f'æˆåŠŸè®°å½• {len(error_ids)} ä¸ªé”™è¯¯',
                    'details': {'recorded_errors': len(recent_errors)}
                }
            else:
                return {
                    'test_name': 'test_basic_logging',
                    'status': 'failed',
                    'message': f'è®°å½•é”™è¯¯æ•°é‡ä¸åŒ¹é…ï¼ŒæœŸæœ› {len(error_ids)}ï¼Œå®é™… {len(recent_errors)}'
                }
                
        except Exception as e:
            return {
                'test_name': 'test_basic_logging',
                'status': 'failed',
                'error': str(e)
            }
    
    def test_error_classification(self) -> Dict[str, Any]:
        """æµ‹è¯•é”™è¯¯åˆ†ç±»åŠŸèƒ½"""
        try:
            # è®°å½•ä¸åŒç±»å‹çš„é”™è¯¯
            test_cases = [
                (ErrorType.SYSTEM, "ç³»ç»Ÿé”™è¯¯æµ‹è¯•"),
                (ErrorType.BUSINESS, "ä¸šåŠ¡é”™è¯¯æµ‹è¯•"),
                (ErrorType.NETWORK, "ç½‘ç»œé”™è¯¯æµ‹è¯•"),
                (ErrorType.DATABASE, "æ•°æ®åº“é”™è¯¯æµ‹è¯•")
            ]
            
            for error_type, message in test_cases:
                self.error_logger.log_error(
                    error_type=error_type,
                    severity=ErrorSeverity.MEDIUM,
                    message=message
                )
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = self.error_logger.get_statistics(hours=1)
            
            # éªŒè¯åˆ†ç±»æ˜¯å¦æ­£ç¡®
            classified_types = [error_type for error_type, count in stats.errors_by_type.items() if count > 0]
            
            expected_types = set(error_type for error_type, _ in test_cases)
            actual_types = set(classified_types)
            
            if expected_types.issubset(actual_types):
                return {
                    'test_name': 'test_error_classification',
                    'status': 'passed',
                    'message': 'é”™è¯¯åˆ†ç±»åŠŸèƒ½æ­£å¸¸',
                    'details': {'classified_types': len(actual_types)}
                }
            else:
                missing_types = expected_types - actual_types
                return {
                    'test_name': 'test_error_classification',
                    'status': 'failed',
                    'message': f'ç¼ºå°‘é”™è¯¯ç±»å‹: {missing_types}'
                }
                
        except Exception as e:
            return {
                'test_name': 'test_error_classification',
                'status': 'failed',
                'error': str(e)
            }
    
    def test_alert_system(self) -> Dict[str, Any]:
        """æµ‹è¯•å‘Šè­¦ç³»ç»Ÿ"""
        try:
            # è®°å½•ä¸€ä¸ªä¸¥é‡é”™è¯¯ä»¥è§¦å‘å‘Šè­¦
            error_id = self.error_logger.log_error(
                error_type=ErrorType.SYSTEM,
                severity=ErrorSeverity.CRITICAL,
                message="æµ‹è¯•ä¸¥é‡é”™è¯¯ - è§¦å‘å‘Šè­¦",
                additional_data={'alert_test': True}
            )
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥å‘é€å‘Šè­¦
            if error_id:
                return {
                    'test_name': 'test_alert_system',
                    'status': 'passed',
                    'message': 'å‘Šè­¦ç³»ç»Ÿæµ‹è¯•å®Œæˆ',
                    'details': {'test_error_id': error_id}
                }
            else:
                return {
                    'test_name': 'test_alert_system',
                    'status': 'failed',
                    'message': 'å‘Šè­¦ç³»ç»Ÿæµ‹è¯•å¤±è´¥'
                }
                
        except Exception as e:
            return {
                'test_name': 'test_alert_system',
                'status': 'failed',
                'error': str(e)
            }
    
    def test_recovery_mechanism(self) -> Dict[str, Any]:
        """æµ‹è¯•æ¢å¤æœºåˆ¶"""
        try:
            def test_recovery_func():
                return "æ¢å¤æˆåŠŸ"
            
            success, result = self.error_logger.attempt_recovery(
                error_type=ErrorType.BUSINESS,
                recovery_func=test_recovery_func
            )
            
            if success and result == "æ¢å¤æˆåŠŸ":
                return {
                    'test_name': 'test_recovery_mechanism',
                    'status': 'passed',
                    'message': 'æ¢å¤æœºåˆ¶æµ‹è¯•æˆåŠŸ',
                    'details': {'recovery_result': result}
                }
            else:
                return {
                    'test_name': 'test_recovery_mechanism',
                    'status': 'failed',
                    'message': f'æ¢å¤æœºåˆ¶æµ‹è¯•å¤±è´¥: {success}, {result}'
                }
                
        except Exception as e:
            return {
                'test_name': 'test_recovery_mechanism',
                'status': 'failed',
                'error': str(e)
            }
    
    def test_statistics_generation(self) -> Dict[str, Any]:
        """æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯ç”Ÿæˆ"""
        try:
            # å…ˆè®°å½•ä¸€äº›é”™è¯¯
            for i in range(5):
                self.error_logger.log_error(
                    error_type=ErrorType.SYSTEM,
                    severity=ErrorSeverity.MEDIUM,
                    message=f"ç»Ÿè®¡æµ‹è¯•é”™è¯¯ {i+1}"
                )
            
            # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
            stats = self.error_logger.get_statistics(hours=1)
            
            if stats.total_errors >= 5:
                return {
                    'test_name': 'test_statistics_generation',
                    'status': 'passed',
                    'message': 'ç»Ÿè®¡ä¿¡æ¯ç”Ÿæˆæ­£å¸¸',
                    'details': {'total_errors': stats.total_errors}
                }
            else:
                return {
                    'test_name': 'test_statistics_generation',
                    'status': 'failed',
                    'message': f'ç»Ÿè®¡ä¿¡æ¯é”™è¯¯æ•°é‡ä¸åŒ¹é…: {stats.total_errors}'
                }
                
        except Exception as e:
            return {
                'test_name': 'test_statistics_generation',
                'status': 'failed',
                'error': str(e)
            }
    
    def test_database_operations(self) -> Dict[str, Any]:
        """æµ‹è¯•æ•°æ®åº“æ“ä½œ"""
        try:
            # è®°å½•é”™è¯¯
            error_id = self.error_logger.log_error(
                error_type=ErrorType.SYSTEM,
                severity=ErrorSeverity.LOW,
                message="æ•°æ®åº“æ“ä½œæµ‹è¯•"
            )
            
            # æµ‹è¯•çŠ¶æ€æ›´æ–°
            success = self.error_logger.resolve_error(error_id, "æµ‹è¯•è§£å†³")
            
            if success:
                return {
                    'test_name': 'test_database_operations',
                    'status': 'passed',
                    'message': 'æ•°æ®åº“æ“ä½œæ­£å¸¸',
                    'details': {'test_error_id': error_id}
                }
            else:
                return {
                    'test_name': 'test_database_operations',
                    'status': 'failed',
                    'message': 'æ•°æ®åº“çŠ¶æ€æ›´æ–°å¤±è´¥'
                }
                
        except Exception as e:
            return {
                'test_name': 'test_database_operations',
                'status': 'failed',
                'error': str(e)
            }
    
    def test_performance_monitoring(self) -> Dict[str, Any]:
        """æµ‹è¯•æ€§èƒ½ç›‘æ§"""
        try:
            # åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨
            monitor = PerformanceMonitor(self.error_logger)
            
            # è®°å½•æ€§èƒ½æ•°æ®
            success = monitor.record_performance(
                operation="test_operation",
                duration=0.1,
                memory_usage=50.0,
                success=True
            )
            
            # è·å–æ€§èƒ½æ‘˜è¦
            summary = monitor.get_performance_summary(hours=1)
            
            if success and 'total_operations' in summary:
                return {
                    'test_name': 'test_performance_monitoring',
                    'status': 'passed',
                    'message': 'æ€§èƒ½ç›‘æ§æ­£å¸¸',
                    'details': summary
                }
            else:
                return {
                    'test_name': 'test_performance_monitoring',
                    'status': 'failed',
                    'message': 'æ€§èƒ½ç›‘æ§æµ‹è¯•å¤±è´¥'
                }
                
        except Exception as e:
            return {
                'test_name': 'test_performance_monitoring',
                'status': 'failed',
                'error': str(e)
            }
    
    def test_configuration_management(self) -> Dict[str, Any]:
        """æµ‹è¯•é…ç½®ç®¡ç†"""
        try:
            # åˆ›å»ºé…ç½®ç®¡ç†å™¨
            config_manager = ConfigurationManager("test_config.json")
            
            # æµ‹è¯•è®¾ç½®å’Œè·å–é…ç½®
            config_manager.set("test_key", "test_value")
            value = config_manager.get("test_key")
            
            # éªŒè¯é…ç½®
            is_valid, errors = config_manager.validate_config()
            
            if value == "test_value" and is_valid:
                return {
                    'test_name': 'test_configuration_management',
                    'status': 'passed',
                    'message': 'é…ç½®ç®¡ç†æ­£å¸¸',
                    'details': {'test_value': value}
                }
            else:
                return {
                    'test_name': 'test_configuration_management',
                    'status': 'failed',
                    'message': f'é…ç½®ç®¡ç†æµ‹è¯•å¤±è´¥: value={value}, valid={is_valid}, errors={errors}'
                }
                
        except Exception as e:
            return {
                'test_name': 'test_configuration_management',
                'status': 'failed',
                'error': str(e)
            }
    
    def test_pattern_recognition(self) -> Dict[str, Any]:
        """æµ‹è¯•æ¨¡å¼è¯†åˆ«"""
        try:
            # åˆ›å»ºæ¨¡å¼è¯†åˆ«å™¨
            recognizer = ErrorPatternRecognizer(self.error_logger)
            
            # å­¦ä¹ æ¨¡å¼
            patterns = recognizer.learn_patterns(days=1)
            
            # é¢„æµ‹ä¸‹ä¸€ä¸ªé”™è¯¯
            predictions = recognizer.predict_next_errors()
            
            if isinstance(patterns, dict) and isinstance(predictions, list):
                return {
                    'test_name': 'test_pattern_recognition',
                    'status': 'passed',
                    'message': 'æ¨¡å¼è¯†åˆ«æ­£å¸¸',
                    'details': {
                        'patterns_found': len(patterns),
                        'predictions_count': len(predictions)
                    }
                }
            else:
                return {
                    'test_name': 'test_pattern_recognition',
                    'status': 'failed',
                    'message': 'æ¨¡å¼è¯†åˆ«æµ‹è¯•å¤±è´¥'
                }
                
        except Exception as e:
            return {
                'test_name': 'test_pattern_recognition',
                'status': 'failed',
                'error': str(e)
            }
    
    def test_async_processing(self) -> Dict[str, Any]:
        """æµ‹è¯•å¼‚æ­¥å¤„ç†"""
        try:
            # è®°å½•å¤šä¸ªé”™è¯¯ä»¥æµ‹è¯•å¼‚æ­¥å¤„ç†
            error_ids = []
            for i in range(10):
                error_id = self.error_logger.log_error(
                    error_type=ErrorType.SYSTEM,
                    severity=ErrorSeverity.LOW,
                    message=f"å¼‚æ­¥å¤„ç†æµ‹è¯•é”™è¯¯ {i+1}",
                    additional_data={'async_test': True}
                )
                error_ids.append(error_id)
            
            # ç­‰å¾…å¼‚æ­¥å¤„ç†å®Œæˆ
            time.sleep(1)
            
            # æ£€æŸ¥é”™è¯¯æ˜¯å¦è¢«å¤„ç†
            recent_errors = self.error_logger.get_recent_errors(len(error_ids))
            
            if len(recent_errors) >= len(error_ids):
                return {
                    'test_name': 'test_async_processing',
                    'status': 'passed',
                    'message': 'å¼‚æ­¥å¤„ç†æ­£å¸¸',
                    'details': {'processed_errors': len(recent_errors)}
                }
            else:
                return {
                    'test_name': 'test_async_processing',
                    'status': 'failed',
                    'message': f'å¼‚æ­¥å¤„ç†é”™è¯¯æ•°é‡ä¸åŒ¹é…: æœŸæœ› {len(error_ids)}, å®é™… {len(recent_errors)}'
                }
                
        except Exception as e:
            return {
                'test_name': 'test_async_processing',
                'status': 'failed',
                'error': str(e)
            }
    
    def generate_test_report(self, output_file: str = "test_report.json") -> bool:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        try:
            with self._lock:
                if not self.test_results:
                    return False
                
                latest_result = self.test_results[-1]
                
                report = {
                    'generated_at': datetime.now().isoformat(),
                    'test_summary': {
                        'total_tests': latest_result['total_tests'],
                        'passed_tests': latest_result['passed_tests'],
                        'failed_tests': latest_result['failed_tests'],
                        'success_rate': round(latest_result['passed_tests'] / latest_result['total_tests'] * 100, 2) if latest_result['total_tests'] > 0 else 0,
                        'overall_status': latest_result['overall_status']
                    },
                    'test_details': latest_result['test_details'],
                    'historical_results': self.test_results[-5:]  # æœ€è¿‘5æ¬¡æµ‹è¯•ç»“æœ
                }
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(report, f, ensure_ascii=False, indent=2)
                
                logging.info(f"æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
                return True
                
        except Exception as e:
            logging.error(f"ç”Ÿæˆæµ‹è¯•æŠ¥å‘Šå¤±è´¥: {str(e)}")
            return False


# æ‰©å±•çš„ç¤ºä¾‹å’Œæ¼”ç¤º
def run_comprehensive_demo():
    """è¿è¡Œç»¼åˆæ¼”ç¤º"""
    print("=== L3é”™è¯¯æ—¥å¿—è®°å½•å™¨ç»¼åˆæ¼”ç¤º ===\n")
    
    # 1. åˆ›å»ºé…ç½®
    config = create_sample_config()
    config['enable_alerts'] = False  # æ¼”ç¤ºæ—¶å…³é—­å‘Šè­¦
    config['async_processing'] = True
    
    # 2. åˆå§‹åŒ–é”™è¯¯æ—¥å¿—è®°å½•å™¨
    error_logger = ErrorLogger(config)
    
    # 3. åˆ›å»ºè¾…åŠ©ç»„ä»¶
    trend_analyzer = ErrorTrendAnalyzer(error_logger)
    performance_monitor = PerformanceMonitor(error_logger)
    config_manager = ConfigurationManager("demo_config.json")
    pattern_recognizer = ErrorPatternRecognizer(error_logger)
    test_suite = AutomatedTestSuite(error_logger)
    
    # 4. æ¼”ç¤ºå„ç§é”™è¯¯åœºæ™¯
    print("1. æ¼”ç¤ºé”™è¯¯è®°å½•åŠŸèƒ½")
    error_scenarios = [
        (ErrorType.SYSTEM, ErrorSeverity.CRITICAL, "ç³»ç»Ÿå†…å­˜ä¸è¶³"),
        (ErrorType.BUSINESS, ErrorSeverity.HIGH, "ä¸šåŠ¡é€»è¾‘éªŒè¯å¤±è´¥"),
        (ErrorType.NETWORK, ErrorSeverity.MEDIUM, "ç½‘ç»œè¿æ¥è¶…æ—¶"),
        (ErrorType.DATABASE, ErrorSeverity.HIGH, "æ•°æ®åº“è¿æ¥å¤±è´¥"),
        (ErrorType.SECURITY, ErrorSeverity.CRITICAL, "æœªæˆæƒè®¿é—®å°è¯•"),
        (ErrorType.PERFORMANCE, ErrorSeverity.MEDIUM, "å“åº”æ—¶é—´è¿‡é•¿"),
        (ErrorType.CONFIGURATION, ErrorSeverity.LOW, "é…ç½®å‚æ•°ç¼ºå¤±")
    ]
    
    for error_type, severity, message in error_scenarios:
        error_id = error_logger.log_error(
            error_type=error_type,
            severity=severity,
            message=message,
            additional_data={
                'demo': True,
                'scenario': 'comprehensive_demo'
            }
        )
        print(f"  è®°å½•é”™è¯¯: {error_id} - {message}")
    
    # 5. æ¼”ç¤ºæ€§èƒ½ç›‘æ§
    print("\n2. æ¼”ç¤ºæ€§èƒ½ç›‘æ§åŠŸèƒ½")
    operations = ["æ•°æ®åº“æŸ¥è¯¢", "APIè°ƒç”¨", "æ–‡ä»¶å¤„ç†", "è®¡ç®—ä»»åŠ¡"]
    for operation in operations:
        import random
        duration = random.uniform(0.1, 2.0)
        success = random.random() > 0.1  # 90%æˆåŠŸç‡
        
        performance_monitor.record_performance(
            operation=operation,
            duration=duration,
            success=success
        )
        print(f"  è®°å½•æ€§èƒ½: {operation} - {duration:.2f}s - {'æˆåŠŸ' if success else 'å¤±è´¥'}")
    
    # 6. æ¼”ç¤ºæ¢å¤æœºåˆ¶
    print("\n3. æ¼”ç¤ºé”™è¯¯æ¢å¤æœºåˆ¶")
    recovery_scenarios = [
        ("ç½‘ç»œé‡è¯•", lambda: "ç½‘ç»œè¿æ¥å·²æ¢å¤"),
        ("ä¸šåŠ¡é™çº§", lambda: "ä½¿ç”¨ç¼“å­˜æ•°æ®"),
        ("æœåŠ¡é‡å¯", lambda: "æœåŠ¡å·²é‡å¯å®Œæˆ")
    ]
    
    for scenario_name, recovery_func in recovery_scenarios:
        success, result = error_logger.attempt_recovery(
            error_type=ErrorType.BUSINESS,
            recovery_func=recovery_func
        )
        print(f"  {scenario_name}: {'æˆåŠŸ' if success else 'å¤±è´¥'} - {result}")
    
    # 7. æ¼”ç¤ºé”™è¯¯è§£å†³æµç¨‹
    print("\n4. æ¼”ç¤ºé”™è¯¯è§£å†³æµç¨‹")
    recent_errors = error_logger.get_recent_errors(3)
    for i, error in enumerate(recent_errors):
        # è§£å†³é”™è¯¯
        success = error_logger.resolve_error(error.error_id, f"æ¼”ç¤ºè§£å†³æ–¹æ¡ˆ {i+1}")
        print(f"  è§£å†³é”™è¯¯ {error.error_id}: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
        
        # å…³é—­é”™è¯¯
        success = error_logger.close_error(error.error_id, "é—®é¢˜å·²éªŒè¯å¹¶å…³é—­")
        print(f"  å…³é—­é”™è¯¯ {error.error_id}: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
    
    # 8. ç­‰å¾…å¼‚æ­¥å¤„ç†å®Œæˆ
    print("\n5. ç­‰å¾…å¼‚æ­¥å¤„ç†å®Œæˆ...")
    time.sleep(2)
    
    # 9. è·å–ç»Ÿè®¡ä¿¡æ¯
    print("\n6. è·å–ç»Ÿè®¡ä¿¡æ¯")
    stats = error_logger.get_statistics(hours=1)
    print(f"  æ€»é”™è¯¯æ•°: {stats.total_errors}")
    print(f"  ä¸¥é‡é”™è¯¯æ•°: {stats.errors_by_severity.get(ErrorSeverity.CRITICAL, 0)}")
    print(f"  ç³»ç»Ÿå½±å“åˆ†æ•°: {stats.system_impact_score:.2f}")
    print(f"  å¹³å‡è§£å†³æ—¶é—´: {stats.avg_resolution_time:.2f}ç§’")
    
    # 10. è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€
    print("\n7. ç³»ç»Ÿå¥åº·çŠ¶æ€")
    health = error_logger.get_system_health()
    print(f"  å¥åº·çŠ¶æ€: {health['status']}")
    print(f"  å¥åº·åˆ†æ•°: {health['health_score']:.1f}")
    print(f"  é”™è¯¯ç‡(æ¯å°æ—¶): {health['error_rate_per_hour']}")
    print(f"  æœ€è¿‘1å°æ—¶ä¸¥é‡é”™è¯¯: {health['critical_errors_last_hour']}")
    
    # 11. æ€§èƒ½ç›‘æ§æ‘˜è¦
    print("\n8. æ€§èƒ½ç›‘æ§æ‘˜è¦")
    perf_summary = performance_monitor.get_performance_summary(hours=1)
    print(f"  æ€»æ“ä½œæ•°: {perf_summary.get('total_operations', 0)}")
    print(f"  æˆåŠŸç‡: {perf_summary.get('success_rate', 0):.1f}%")
    print(f"  å¹³å‡å“åº”æ—¶é—´: {perf_summary.get('response_time', {}).get('avg', 0):.3f}ç§’")
    
    # 12. é”™è¯¯è¶‹åŠ¿åˆ†æ
    print("\n9. é”™è¯¯è¶‹åŠ¿åˆ†æ")
    trends = trend_analyzer.analyze_error_trends(days=1)
    if trends:
        print(f"  é”™è¯¯å¢é•¿ç‡: {trends.get('error_growth_rate', 0):.1f}%")
        if trends.get('peak_error_day'):
            peak = trends['peak_error_day']
            print(f"  å³°å€¼é”™è¯¯æ—¥: {peak['date']} ({peak['error_count']}ä¸ªé”™è¯¯)")
    
    # 13. é”™è¯¯æ¨¡å¼è¯†åˆ«
    print("\n10. é”™è¯¯æ¨¡å¼è¯†åˆ«")
    patterns = pattern_recognizer.learn_patterns(days=1)
    if patterns:
        recurring_count = len(patterns.get('recurring_errors', {}))
        print(f"  å‘ç°é‡å¤é”™è¯¯æ¨¡å¼: {recurring_count}ä¸ª")
    
    predictions = pattern_recognizer.predict_next_errors()
    print(f"  é¢„æµ‹å¯èƒ½å‘ç”Ÿçš„é”™è¯¯: {len(predictions)}ä¸ª")
    
    # 14. è¿è¡Œè‡ªåŠ¨åŒ–æµ‹è¯•
    print("\n11. è¿è¡Œè‡ªåŠ¨åŒ–æµ‹è¯•")
    test_results = test_suite.run_all_tests()
    print(f"  æµ‹è¯•çŠ¶æ€: {test_results['overall_status']}")
    print(f"  é€šè¿‡æµ‹è¯•: {test_results['passed_tests']}/{test_results['total_tests']}")
    print(f"  æˆåŠŸç‡: {test_results['passed_tests']/test_results['total_tests']*100:.1f}%" if test_results['total_tests'] > 0 else "  æ— æµ‹è¯•")
    
    # 15. å¯¼å‡ºæŠ¥å‘Š
    print("\n12. å¯¼å‡ºæŠ¥å‘Š")
    error_logger.export_error_report("comprehensive_demo_report.json", hours=1)
    test_suite.generate_test_report("demo_test_report.json")
    print("  é”™è¯¯æŠ¥å‘Šå·²å¯¼å‡º: comprehensive_demo_report.json")
    print("  æµ‹è¯•æŠ¥å‘Šå·²å¯¼å‡º: demo_test_report.json")
    
    # 16. æ¸…ç†æ¼”ç¤ºæ•°æ®
    print("\n13. æ¸…ç†æ¼”ç¤ºæ•°æ®")
    cleaned_count = error_logger.cleanup_old_logs(days=0)  # æ¸…ç†æ‰€æœ‰æ¼”ç¤ºæ•°æ®
    print(f"  æ¸…ç†äº† {cleaned_count} æ¡æ¼”ç¤ºè®°å½•")
    
    # 17. å…³é—­ç³»ç»Ÿ
    print("\n14. å…³é—­ç³»ç»Ÿ")
    error_logger.shutdown()
    print("  é”™è¯¯æ—¥å¿—è®°å½•å™¨å·²å…³é—­")
    
    print("\n=== ç»¼åˆæ¼”ç¤ºå®Œæˆ ===")


if __name__ == "__main__":
    # è¿è¡ŒåŸºç¡€ç¤ºä¾‹
    print("è¿è¡ŒåŸºç¡€ç¤ºä¾‹...")
    
    # è¿è¡Œç»¼åˆæ¼”ç¤º
    run_comprehensive_demo()