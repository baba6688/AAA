"""
V3æ¨¡å‹è¯„ä¼°å™¨
==============

ä¸€ä¸ªç»¼åˆæ€§çš„æ¨¡å‹è¯„ä¼°å™¨ï¼Œæ”¯æŒåˆ†ç±»å’Œå›å½’æ¨¡å‹çš„å…¨é¢æ€§èƒ½è¯„ä¼°ã€‚

åŠŸèƒ½ç‰¹æ€§:
- å¤šæŒ‡æ ‡è¯„ä¼°ï¼ˆå‡†ç¡®ç‡ã€å¬å›ç‡ã€F1ç­‰åˆ†ç±»æŒ‡æ ‡ï¼‰
- æ··æ·†çŸ©é˜µå’ŒROCæ›²çº¿å¯è§†åŒ–
- å›å½’è¯„ä¼°æŒ‡æ ‡ï¼ˆMAEã€MSEã€R2ç­‰ï¼‰
- åˆ†ç±»å’Œå›å½’æ¨¡å‹è¯„ä¼°
- æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ
- è¯„ä¼°ç»“æœå¯è§†åŒ–
- è¯„ä¼°æŠ¥å‘Šç”Ÿæˆ
- è¯„ä¼°ç»“æœå­˜å‚¨
- è¯„ä¼°ç»“æœè§£é‡Š


ç‰ˆæœ¬: 3.0
æ—¥æœŸ: 2025-11-05
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve, precision_recall_curve,
    confusion_matrix, classification_report,
    mean_absolute_error, mean_squared_error, r2_score,
    mean_absolute_percentage_error, explained_variance_score
)
from typing import Dict, List, Tuple, Union, Optional, Any
import json
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')


class ModelEvaluator:
    """
    V3æ¨¡å‹è¯„ä¼°å™¨
    
    æä¾›å…¨é¢çš„æ¨¡å‹æ€§èƒ½è¯„ä¼°åŠŸèƒ½ï¼Œæ”¯æŒåˆ†ç±»å’Œå›å½’ä»»åŠ¡ã€‚
    """
    
    def __init__(self, task_type: str = "classification", save_path: str = "./evaluation_results"):
        """
        åˆå§‹åŒ–æ¨¡å‹è¯„ä¼°å™¨
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹ï¼Œ"classification" æˆ– "regression"
            save_path: ç»“æœä¿å­˜è·¯å¾„
        """
        self.task_type = task_type.lower()
        self.save_path = save_path
        self.evaluation_results = {}
        self.model_comparisons = {}
        
        # ç¡®ä¿ä¿å­˜è·¯å¾„å­˜åœ¨
        os.makedirs(save_path, exist_ok=True)
        
        # è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        print(f"âœ… æ¨¡å‹è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“Š ä»»åŠ¡ç±»å‹: {self.task_type}")
        print(f"ğŸ’¾ ä¿å­˜è·¯å¾„: {self.save_path}")
    
    def evaluate_classification(self, 
                              y_true: np.ndarray, 
                              y_pred: np.ndarray, 
                              y_prob: Optional[np.ndarray] = None,
                              model_name: str = "Model",
                              labels: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        åˆ†ç±»æ¨¡å‹è¯„ä¼°
        
        Args:
            y_true: çœŸå®æ ‡ç­¾
            y_pred: é¢„æµ‹æ ‡ç­¾
            y_prob: é¢„æµ‹æ¦‚ç‡ï¼ˆå¯é€‰ï¼‰
            model_name: æ¨¡å‹åç§°
            labels: ç±»åˆ«æ ‡ç­¾åç§°
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        print(f"\nğŸ” å¼€å§‹è¯„ä¼°åˆ†ç±»æ¨¡å‹: {model_name}")
        
        # åŸºæœ¬åˆ†ç±»æŒ‡æ ‡
        accuracy = accuracy_score(y_true, y_pred)
        precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)
        precision_micro = precision_score(y_true, y_pred, average='micro', zero_division=0)
        precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)
        
        recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)
        recall_micro = recall_score(y_true, y_pred, average='micro', zero_division=0)
        recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)
        
        f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)
        f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)
        f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)
        
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred)
        
        # ROC-AUCè¯„åˆ†ï¼ˆä»…åœ¨äºŒåˆ†ç±»æˆ–å¤šåˆ†ç±»ä¸”æä¾›æ¦‚ç‡æ—¶ï¼‰
        roc_auc = None
        if y_prob is not None:
            try:
                if len(np.unique(y_true)) == 2:  # äºŒåˆ†ç±»
                    roc_auc = roc_auc_score(y_true, y_prob[:, 1] if y_prob.shape[1] > 1 else y_prob)
                else:  # å¤šåˆ†ç±»
                    roc_auc = roc_auc_score(y_true, y_prob, multi_class='ovr', average='macro')
            except Exception as e:
                print(f"âš ï¸  ROC-AUCè®¡ç®—å¤±è´¥: {e}")
        
        # æ•´ç†è¯„ä¼°ç»“æœ
        results = {
            "model_name": model_name,
            "task_type": "classification",
            "accuracy": accuracy,
            "precision": {
                "macro": precision_macro,
                "micro": precision_micro,
                "weighted": precision_weighted
            },
            "recall": {
                "macro": recall_macro,
                "micro": recall_micro,
                "weighted": recall_weighted
            },
            "f1_score": {
                "macro": f1_macro,
                "micro": f1_micro,
                "weighted": f1_weighted
            },
            "confusion_matrix": cm.tolist(),
            "roc_auc": roc_auc,
            "classification_report": classification_report(y_true, y_pred, output_dict=True, zero_division=0),
            "timestamp": datetime.now().isoformat()
        }
        
        # å­˜å‚¨ç»“æœ
        self.evaluation_results[model_name] = results
        
        # æ‰“å°å…³é”®æŒ‡æ ‡
        print(f"ğŸ“ˆ å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"ğŸ¯ ç²¾ç¡®ç‡ (macro): {precision_macro:.4f}")
        print(f"ğŸ”„ å¬å›ç‡ (macro): {recall_macro:.4f}")
        print(f"âš–ï¸ F1åˆ†æ•° (macro): {f1_macro:.4f}")
        if roc_auc is not None:
            print(f"ğŸ“Š ROC-AUC: {roc_auc:.4f}")
        
        return results
    
    def evaluate_regression(self, 
                          y_true: np.ndarray, 
                          y_pred: np.ndarray,
                          model_name: str = "Model") -> Dict[str, Any]:
        """
        å›å½’æ¨¡å‹è¯„ä¼°
        
        Args:
            y_true: çœŸå®å€¼
            y_pred: é¢„æµ‹å€¼
            model_name: æ¨¡å‹åç§°
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        print(f"\nğŸ” å¼€å§‹è¯„ä¼°å›å½’æ¨¡å‹: {model_name}")
        
        # åŸºæœ¬å›å½’æŒ‡æ ‡
        mae = mean_absolute_error(y_true, y_pred)
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_true, y_pred)
        mape = mean_absolute_percentage_error(y_true, y_pred)
        explained_var = explained_variance_score(y_true, y_pred)
        
        # æ®‹å·®åˆ†æ
        residuals = y_true - y_pred
        residual_mean = np.mean(residuals)
        residual_std = np.std(residuals)
        
        # æ•´ç†è¯„ä¼°ç»“æœ
        results = {
            "model_name": model_name,
            "task_type": "regression",
            "mae": mae,
            "mse": mse,
            "rmse": rmse,
            "r2_score": r2,
            "mape": mape,
            "explained_variance": explained_var,
            "residual_mean": residual_mean,
            "residual_std": residual_std,
            "residuals": residuals.tolist(),
            "timestamp": datetime.now().isoformat()
        }
        
        # å­˜å‚¨ç»“æœ
        self.evaluation_results[model_name] = results
        
        # æ‰“å°å…³é”®æŒ‡æ ‡
        print(f"ğŸ“Š MAE: {mae:.4f}")
        print(f"ğŸ“Š MSE: {mse:.4f}")
        print(f"ğŸ“Š RMSE: {rmse:.4f}")
        print(f"ğŸ“ˆ RÂ²: {r2:.4f}")
        print(f"ğŸ“Š MAPE: {mape:.4f}")
        print(f"ğŸ“Š è§£é‡Šæ–¹å·®: {explained_var:.4f}")
        
        return results
    
    def plot_confusion_matrix(self, model_name: str, save_plot: bool = True) -> None:
        """
        ç»˜åˆ¶æ··æ·†çŸ©é˜µ
        
        Args:
            model_name: æ¨¡å‹åç§°
            save_plot: æ˜¯å¦ä¿å­˜å›¾è¡¨
        """
        if model_name not in self.evaluation_results:
            print(f"âŒ æ¨¡å‹ {model_name} çš„è¯„ä¼°ç»“æœä¸å­˜åœ¨")
            return
        
        results = self.evaluation_results[model_name]
        if results["task_type"] != "classification":
            print(f"âŒ æ¨¡å‹ {model_name} ä¸æ˜¯åˆ†ç±»ä»»åŠ¡")
            return
        
        cm = np.array(results["confusion_matrix"])
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=['é¢„æµ‹è´Ÿç±»', 'é¢„æµ‹æ­£ç±»'] if cm.shape[0] == 2 else None,
                   yticklabels=['çœŸå®è´Ÿç±»', 'çœŸå®æ­£ç±»'] if cm.shape[0] == 2 else None)
        plt.title(f'æ··æ·†çŸ©é˜µ - {model_name}')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        
        if save_plot:
            filepath = os.path.join(self.save_path, f"confusion_matrix_{model_name}.png")
            plt.savefig(filepath, dpi=300, bbox_inches='tight')
            print(f"ğŸ’¾ æ··æ·†çŸ©é˜µå·²ä¿å­˜: {filepath}")
        
        plt.show()
    
    def plot_roc_curve(self, model_name: str, save_plot: bool = True) -> None:
        """
        ç»˜åˆ¶ROCæ›²çº¿
        
        Args:
            model_name: æ¨¡å‹åç§°
            save_plot: æ˜¯å¦ä¿å­˜å›¾è¡¨
        """
        if model_name not in self.evaluation_results:
            print(f"âŒ æ¨¡å‹ {model_name} çš„è¯„ä¼°ç»“æœä¸å­˜åœ¨")
            return
        
        results = self.evaluation_results[model_name]
        if results["task_type"] != "classification":
            print(f"âŒ æ¨¡å‹ {model_name} ä¸æ˜¯åˆ†ç±»ä»»åŠ¡")
            return
        
        if results["roc_auc"] is None:
            print(f"âŒ æ¨¡å‹ {model_name} ç¼ºå°‘æ¦‚ç‡é¢„æµ‹ï¼Œæ— æ³•ç»˜åˆ¶ROCæ›²çº¿")
            return
        
        # è¿™é‡Œéœ€è¦åŸå§‹çš„æ¦‚ç‡é¢„æµ‹æ•°æ®æ¥ç»˜åˆ¶ROCæ›²çº¿
        # åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œåº”è¯¥ä¿å­˜è¿™äº›æ•°æ®
        print(f"ğŸ“Š {model_name} çš„ROC-AUC: {results['roc_auc']:.4f}")
    
    def plot_regression_results(self, model_name: str, save_plot: bool = True) -> None:
        """
        ç»˜åˆ¶å›å½’ç»“æœ
        
        Args:
            model_name: æ¨¡å‹åç§°
            save_plot: æ˜¯å¦ä¿å­˜å›¾è¡¨
        """
        if model_name not in self.evaluation_results:
            print(f"âŒ æ¨¡å‹ {model_name} çš„è¯„ä¼°ç»“æœä¸å­˜åœ¨")
            return
        
        results = self.evaluation_results[model_name]
        if results["task_type"] != "regression":
            print(f"âŒ æ¨¡å‹ {model_name} ä¸æ˜¯å›å½’ä»»åŠ¡")
            return
        
        residuals = np.array(results["residuals"])
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        # æ®‹å·®åˆ†å¸ƒå›¾
        axes[0].hist(residuals, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0].set_title(f'æ®‹å·®åˆ†å¸ƒ - {model_name}')
        axes[0].set_xlabel('æ®‹å·®')
        axes[0].set_ylabel('é¢‘æ¬¡')
        axes[0].axvline(x=0, color='red', linestyle='--', alpha=0.7)
        
        # Q-Qå›¾ï¼ˆç®€åŒ–ç‰ˆï¼‰
        from scipy import stats
        stats.probplot(residuals, dist="norm", plot=axes[1])
        axes[1].set_title(f'æ®‹å·®Q-Qå›¾ - {model_name}')
        
        plt.tight_layout()
        
        if save_plot:
            filepath = os.path.join(self.save_path, f"regression_results_{model_name}.png")
            plt.savefig(filepath, dpi=300, bbox_inches='tight')
            print(f"ğŸ’¾ å›å½’ç»“æœå›¾å·²ä¿å­˜: {filepath}")
        
        plt.show()
    
    def compare_models(self, model_names: List[str], metric: str = "accuracy") -> pd.DataFrame:
        """
        æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„æ€§èƒ½
        
        Args:
            model_names: æ¨¡å‹åç§°åˆ—è¡¨
            metric: æ¯”è¾ƒæŒ‡æ ‡
            
        Returns:
            æ¯”è¾ƒç»“æœDataFrame
        """
        print(f"\nğŸ”„ å¼€å§‹æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ")
        print(f"ğŸ“Š æ¯”è¾ƒæŒ‡æ ‡: {metric}")
        
        comparison_data = []
        
        for model_name in model_names:
            if model_name not in self.evaluation_results:
                print(f"âš ï¸  æ¨¡å‹ {model_name} çš„è¯„ä¼°ç»“æœä¸å­˜åœ¨ï¼Œè·³è¿‡")
                continue
            
            results = self.evaluation_results[model_name]
            
            if metric in results:
                comparison_data.append({
                    "Model": model_name,
                    "Metric": metric,
                    "Value": results[metric]
                })
            elif metric in results.get("precision", {}):
                comparison_data.append({
                    "Model": model_name,
                    "Metric": f"{metric}_macro",
                    "Value": results["precision"][metric]
                })
            elif metric in results.get("recall", {}):
                comparison_data.append({
                    "Model": model_name,
                    "Metric": f"{metric}_macro",
                    "Value": results["recall"][metric]
                })
            elif metric in results.get("f1_score", {}):
                comparison_data.append({
                    "Model": model_name,
                    "Metric": f"{metric}_macro",
                    "Value": results["f1_score"][metric]
                })
        
        if not comparison_data:
            print(f"âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ¯”è¾ƒæ•°æ®")
            return pd.DataFrame()
        
        comparison_df = pd.DataFrame(comparison_data)
        self.model_comparisons[metric] = comparison_df
        
        # æ‰“å°æ¯”è¾ƒç»“æœ
        print("\nğŸ“Š æ¨¡å‹æ€§èƒ½æ¯”è¾ƒç»“æœ:")
        print(comparison_df.to_string(index=False))
        
        # å¯è§†åŒ–æ¯”è¾ƒç»“æœ
        self._plot_model_comparison(comparison_df, metric)
        
        return comparison_df
    
    def _plot_model_comparison(self, comparison_df: pd.DataFrame, metric: str) -> None:
        """
        ç»˜åˆ¶æ¨¡å‹æ¯”è¾ƒå›¾
        
        Args:
            comparison_df: æ¯”è¾ƒæ•°æ®
            metric: æŒ‡æ ‡åç§°
        """
        plt.figure(figsize=(10, 6))
        
        bars = plt.bar(comparison_df["Model"], comparison_df["Value"], 
                      color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])
        
        plt.title(f'æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ - {metric}', fontsize=16, fontweight='bold')
        plt.xlabel('æ¨¡å‹', fontsize=12)
        plt.ylabel(f'{metric} å€¼', fontsize=12)
        plt.xticks(rotation=45)
        
        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars:
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{height:.4f}', ha='center', va='bottom', fontweight='bold')
        
        plt.grid(axis='y', alpha=0.3)
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        filepath = os.path.join(self.save_path, f"model_comparison_{metric}.png")
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        print(f"ğŸ’¾ æ¨¡å‹æ¯”è¾ƒå›¾å·²ä¿å­˜: {filepath}")
        
        plt.show()
    
    def generate_report(self, model_name: str, output_format: str = "html") -> str:
        """
        ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        
        Args:
            model_name: æ¨¡å‹åç§°
            output_format: è¾“å‡ºæ ¼å¼ï¼Œ"html" æˆ– "json"
            
        Returns:
            æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        if model_name not in self.evaluation_results:
            print(f"âŒ æ¨¡å‹ {model_name} çš„è¯„ä¼°ç»“æœä¸å­˜åœ¨")
            return ""
        
        results = self.evaluation_results[model_name]
        
        if output_format.lower() == "json":
            # JSONæ ¼å¼æŠ¥å‘Š
            report_data = {
                "model_name": model_name,
                "evaluation_summary": self._generate_summary(results),
                "detailed_results": results,
                "interpretation": self._interpret_results(results),
                "recommendations": self._generate_recommendations(results)
            }
            
            filepath = os.path.join(self.save_path, f"evaluation_report_{model_name}.json")
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        elif output_format.lower() == "html":
            # HTMLæ ¼å¼æŠ¥å‘Š
            html_content = self._generate_html_report(results)
            filepath = os.path.join(self.save_path, f"evaluation_report_{model_name}.html")
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(html_content)
        
        print(f"ğŸ“„ è¯„ä¼°æŠ¥å‘Šå·²ç”Ÿæˆ: {filepath}")
        return filepath
    
    def _generate_summary(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """
        ç”Ÿæˆè¯„ä¼°æ‘˜è¦
        
        Args:
            results: è¯„ä¼°ç»“æœ
            
        Returns:
            æ‘˜è¦ä¿¡æ¯
        """
        if results["task_type"] == "classification":
            return {
                "æ€»ä½“è¡¨ç°": "è‰¯å¥½" if results["accuracy"] > 0.8 else "ä¸€èˆ¬" if results["accuracy"] > 0.6 else "è¾ƒå·®",
                "å‡†ç¡®ç‡": f"{results['accuracy']:.4f}",
                "F1åˆ†æ•°": f"{results['f1_score']['macro']:.4f}",
                "ROC-AUC": f"{results['roc_auc']:.4f}" if results['roc_auc'] else "N/A"
            }
        else:
            return {
                "æ€»ä½“è¡¨ç°": "è‰¯å¥½" if results["r2_score"] > 0.8 else "ä¸€èˆ¬" if results["r2_score"] > 0.6 else "è¾ƒå·®",
                "RÂ²åˆ†æ•°": f"{results['r2_score']:.4f}",
                "RMSE": f"{results['rmse']:.4f}",
                "MAE": f"{results['mae']:.4f}"
            }
    
    def _interpret_results(self, results: Dict[str, Any]) -> List[str]:
        """
        è§£é‡Šè¯„ä¼°ç»“æœ
        
        Args:
            results: è¯„ä¼°ç»“æœ
            
        Returns:
            è§£é‡Šåˆ—è¡¨
        """
        interpretations = []
        
        if results["task_type"] == "classification":
            accuracy = results["accuracy"]
            f1_macro = results["f1_score"]["macro"]
            
            if accuracy > 0.9:
                interpretations.append("ğŸ‰ æ¨¡å‹è¡¨ç°ä¼˜ç§€ï¼Œå‡†ç¡®ç‡è¶…è¿‡90%")
            elif accuracy > 0.8:
                interpretations.append("ğŸ‘ æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼Œå‡†ç¡®ç‡è¶…è¿‡80%")
            elif accuracy > 0.6:
                interpretations.append("âš ï¸ æ¨¡å‹è¡¨ç°ä¸€èˆ¬ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–")
            else:
                interpretations.append("âŒ æ¨¡å‹è¡¨ç°è¾ƒå·®ï¼Œéœ€è¦é‡æ–°è®¾è®¡")
            
            if f1_macro < accuracy - 0.1:
                interpretations.append("ğŸ“Š F1åˆ†æ•°ä½äºå‡†ç¡®ç‡ï¼Œå¯èƒ½å­˜åœ¨ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜")
            
            if results.get("roc_auc"):
                if results["roc_auc"] > 0.9:
                    interpretations.append("ğŸ“ˆ ROC-AUCä¼˜ç§€ï¼Œæ¨¡å‹å…·æœ‰å¾ˆå¼ºçš„åŒºåˆ†èƒ½åŠ›")
                elif results["roc_auc"] > 0.8:
                    interpretations.append("ğŸ“Š ROC-AUCè‰¯å¥½ï¼Œæ¨¡å‹å…·æœ‰è¾ƒå¥½çš„åŒºåˆ†èƒ½åŠ›")
        
        else:
            r2 = results["r2_score"]
            rmse = results["rmse"]
            
            if r2 > 0.9:
                interpretations.append("ğŸ‰ æ¨¡å‹æ‹Ÿåˆä¼˜ç§€ï¼ŒRÂ²è¶…è¿‡90%")
            elif r2 > 0.8:
                interpretations.append("ğŸ‘ æ¨¡å‹æ‹Ÿåˆè‰¯å¥½ï¼ŒRÂ²è¶…è¿‡80%")
            elif r2 > 0.6:
                interpretations.append("âš ï¸ æ¨¡å‹æ‹Ÿåˆä¸€èˆ¬ï¼Œå»ºè®®å¢åŠ ç‰¹å¾æˆ–è°ƒæ•´æ¨¡å‹")
            else:
                interpretations.append("âŒ æ¨¡å‹æ‹Ÿåˆè¾ƒå·®ï¼Œéœ€è¦é‡æ–°å»ºæ¨¡")
            
            if abs(results["residual_mean"]) > rmse * 0.1:
                interpretations.append("ğŸ“Š æ®‹å·®å­˜åœ¨ç³»ç»Ÿæ€§åå·®ï¼Œå»ºè®®æ£€æŸ¥æ¨¡å‹å‡è®¾")
        
        return interpretations
    
    def _generate_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """
        ç”Ÿæˆæ”¹è¿›å»ºè®®
        
        Args:
            results: è¯„ä¼°ç»“æœ
            
        Returns:
            å»ºè®®åˆ—è¡¨
        """
        recommendations = []
        
        if results["task_type"] == "classification":
            accuracy = results["accuracy"]
            
            if accuracy < 0.7:
                recommendations.extend([
                    "ğŸ”§ å°è¯•ç‰¹å¾å·¥ç¨‹ï¼Œå¢åŠ æ›´å¤šæœ‰æ„ä¹‰çš„ç‰¹å¾",
                    "ğŸ¯ è°ƒæ•´æ¨¡å‹è¶…å‚æ•°ï¼Œä½¿ç”¨ç½‘æ ¼æœç´¢æˆ–è´å¶æ–¯ä¼˜åŒ–",
                    "ğŸ“Š æ£€æŸ¥æ•°æ®è´¨é‡ï¼Œå¤„ç†å¼‚å¸¸å€¼å’Œç¼ºå¤±å€¼",
                    "âš–ï¸ è€ƒè™‘å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œä½¿ç”¨é‡é‡‡æ ·æˆ–æƒé‡è°ƒæ•´"
                ])
            
            if results.get("roc_auc") and results["roc_auc"] < 0.8:
                recommendations.append("ğŸ“ˆ ä¼˜åŒ–åˆ†ç±»é˜ˆå€¼ï¼Œæé«˜çœŸæ­£ä¾‹ç‡")
            
            f1_scores = results["f1_score"]
            if f1_scores["macro"] < f1_scores["micro"]:
                recommendations.append("âš–ï¸ å…³æ³¨å°‘æ•°ç±»åˆ«çš„é¢„æµ‹æ€§èƒ½")
        
        else:
            r2 = results["r2_score"]
            
            if r2 < 0.7:
                recommendations.extend([
                    "ğŸ”§ å¢åŠ æ›´å¤šç›¸å…³ç‰¹å¾æˆ–è¿›è¡Œç‰¹å¾äº¤äº’",
                    "ğŸ¯ å°è¯•ä¸åŒçš„æ¨¡å‹ç®—æ³•ï¼Œå¦‚é›†æˆæ–¹æ³•",
                    "ğŸ“Š æ£€æŸ¥ç‰¹å¾ä¸ç›®æ ‡å˜é‡çš„çº¿æ€§å…³ç³»",
                    "ğŸ” åˆ†ææ®‹å·®æ¨¡å¼ï¼Œè€ƒè™‘éçº¿æ€§å»ºæ¨¡"
                ])
            
            if results["mape"] > 0.1:
                recommendations.append("ğŸ“Š MAPEè¾ƒé«˜ï¼Œè€ƒè™‘å¯¹ç›®æ ‡å˜é‡è¿›è¡Œå˜æ¢")
        
        return recommendations
    
    def _generate_html_report(self, results: Dict[str, Any]) -> str:
        """
        ç”ŸæˆHTMLæŠ¥å‘Š
        
        Args:
            results: è¯„ä¼°ç»“æœ
            
        Returns:
            HTMLå†…å®¹
        """
        summary = self._generate_summary(results)
        interpretations = self._interpret_results(results)
        recommendations = self._generate_recommendations(results)
        
        html_template = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ¨¡å‹è¯„ä¼°æŠ¥å‘Š - {results['model_name']}</title>
    <style>
        body {{ font-family: 'Microsoft YaHei', Arial, sans-serif; margin: 40px; background-color: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background-color: white; padding: 30px; border-radius: 10px; box-shadow: 0 0 20px rgba(0,0,0,0.1); }}
        .header {{ text-align: center; color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 20px; margin-bottom: 30px; }}
        .section {{ margin: 25px 0; padding: 20px; border-radius: 8px; }}
        .summary {{ background-color: #ecf0f1; }}
        .interpretation {{ background-color: #e8f5e8; }}
        .recommendations {{ background-color: #fff3cd; }}
        .metrics {{ background-color: #f8f9fa; }}
        .metric-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; }}
        .metric-card {{ background: white; padding: 15px; border-radius: 5px; border-left: 4px solid #3498db; }}
        .metric-value {{ font-size: 24px; font-weight: bold; color: #2c3e50; }}
        .metric-label {{ color: #7f8c8d; font-size: 14px; }}
        .interpretation-item, .recommendation-item {{ margin: 10px 0; padding: 10px; background: white; border-radius: 5px; }}
        .timestamp {{ text-align: center; color: #7f8c8d; margin-top: 30px; font-size: 12px; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“Š æ¨¡å‹è¯„ä¼°æŠ¥å‘Š</h1>
            <h2>{results['model_name']}</h2>
            <p>ä»»åŠ¡ç±»å‹: {results['task_type'].title()}</p>
        </div>
        
        <div class="section summary">
            <h3>ğŸ“ˆ è¯„ä¼°æ‘˜è¦</h3>
            <div class="metric-grid">
                {self._generate_metric_cards(summary)}
            </div>
        </div>
        
        <div class="section metrics">
            <h3>ğŸ“Š è¯¦ç»†æŒ‡æ ‡</h3>
            {self._generate_detailed_metrics(results)}
        </div>
        
        <div class="section interpretation">
            <h3>ğŸ’¡ ç»“æœè§£é‡Š</h3>
            {self._generate_interpretation_html(interpretations)}
        </div>
        
        <div class="section recommendations">
            <h3>ğŸ¯ æ”¹è¿›å»ºè®®</h3>
            {self._generate_recommendations_html(recommendations)}
        </div>
        
        <div class="timestamp">
            <p>ç”Ÿæˆæ—¶é—´: {results['timestamp']}</p>
        </div>
    </div>
</body>
</html>
        """
        return html_template
    
    def _generate_metric_cards(self, summary: Dict[str, str]) -> str:
        """ç”ŸæˆæŒ‡æ ‡å¡ç‰‡HTML"""
        cards = ""
        for key, value in summary.items():
            cards += f"""
            <div class="metric-card">
                <div class="metric-value">{value}</div>
                <div class="metric-label">{key}</div>
            </div>
            """
        return cards
    
    def _generate_detailed_metrics(self, results: Dict[str, Any]) -> str:
        """ç”Ÿæˆè¯¦ç»†æŒ‡æ ‡HTML"""
        html = "<div class='metric-grid'>"
        
        if results["task_type"] == "classification":
            html += f"""
            <div class="metric-card"><div class="metric-value">{results['accuracy']:.4f}</div><div class="metric-label">å‡†ç¡®ç‡</div></div>
            <div class="metric-card"><div class="metric-value">{results['f1_score']['macro']:.4f}</div><div class="metric-label">F1åˆ†æ•° (macro)</div></div>
            <div class="metric-card"><div class="metric-value">{results['precision']['macro']:.4f}</div><div class="metric-label">ç²¾ç¡®ç‡ (macro)</div></div>
            <div class="metric-card"><div class="metric-value">{results['recall']['macro']:.4f}</div><div class="metric-label">å¬å›ç‡ (macro)</div></div>
            """
            if results.get('roc_auc'):
                html += f"<div class='metric-card'><div class='metric-value'>{results['roc_auc']:.4f}</div><div class='metric-label'>ROC-AUC</div></div>"
        else:
            html += f"""
            <div class="metric-card"><div class="metric-value">{results['r2_score']:.4f}</div><div class="metric-label">RÂ²åˆ†æ•°</div></div>
            <div class="metric-card"><div class="metric-value">{results['rmse']:.4f}</div><div class="metric-label">RMSE</div></div>
            <div class="metric-card"><div class="metric-value">{results['mae']:.4f}</div><div class="metric-label">MAE</div></div>
            <div class="metric-card"><div class="metric-value">{results['mape']:.4f}</div><div class="metric-label">MAPE</div></div>
            """
        
        html += "</div>"
        return html
    
    def _generate_interpretation_html(self, interpretations: List[str]) -> str:
        """ç”Ÿæˆè§£é‡ŠHTML"""
        html = ""
        for interpretation in interpretations:
            html += f"<div class='interpretation-item'>{interpretation}</div>"
        return html
    
    def _generate_recommendations_html(self, recommendations: List[str]) -> str:
        """ç”Ÿæˆå»ºè®®HTML"""
        html = ""
        for recommendation in recommendations:
            html += f"<div class='recommendation-item'>{recommendation}</div>"
        return html
    
    def save_results(self, filepath: Optional[str] = None) -> str:
        """
        ä¿å­˜è¯„ä¼°ç»“æœ
        
        Args:
            filepath: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if not filepath:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = os.path.join(self.save_path, f"evaluation_results_{timestamp}.json")
        
        save_data = {
            "evaluation_results": self.evaluation_results,
            "model_comparisons": self.model_comparisons,
            "timestamp": datetime.now().isoformat()
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜: {filepath}")
        return filepath
    
    def load_results(self, filepath: str) -> None:
        """
        åŠ è½½è¯„ä¼°ç»“æœ
        
        Args:
            filepath: æ–‡ä»¶è·¯å¾„
        """
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            self.evaluation_results = data.get("evaluation_results", {})
            self.model_comparisons = data.get("model_comparisons", {})
            
            print(f"ğŸ“‚ è¯„ä¼°ç»“æœå·²åŠ è½½: {filepath}")
            print(f"ğŸ“Š å·²åŠ è½½ {len(self.evaluation_results)} ä¸ªæ¨¡å‹çš„è¯„ä¼°ç»“æœ")
            
        except Exception as e:
            print(f"âŒ åŠ è½½è¯„ä¼°ç»“æœå¤±è´¥: {e}")
    
    def get_best_model(self, metric: str = "accuracy") -> Tuple[str, float]:
        """
        è·å–æœ€ä½³æ¨¡å‹
        
        Args:
            metric: æ¯”è¾ƒæŒ‡æ ‡
            
        Returns:
            (æœ€ä½³æ¨¡å‹åç§°, æŒ‡æ ‡å€¼)
        """
        best_model = None
        best_score = float('-inf')
        
        for model_name, results in self.evaluation_results.items():
            if metric in results:
                score = results[metric]
            elif metric in results.get("precision", {}):
                score = results["precision"][metric]
            elif metric in results.get("recall", {}):
                score = results["recall"][metric]
            elif metric in results.get("f1_score", {}):
                score = results["f1_score"][metric]
            else:
                continue
            
            if score > best_score:
                best_score = score
                best_model = model_name
        
        if best_model:
            print(f"ğŸ† æœ€ä½³æ¨¡å‹: {best_model} ({metric}: {best_score:.4f})")
            return best_model, best_score
        else:
            print(f"âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ¨¡å‹æ¯”è¾ƒç»“æœ")
            return None, 0.0
    
    def get_evaluation_summary(self) -> pd.DataFrame:
        """
        è·å–æ‰€æœ‰æ¨¡å‹çš„è¯„ä¼°æ‘˜è¦
        
        Returns:
            æ‘˜è¦DataFrame
        """
        summary_data = []
        
        for model_name, results in self.evaluation_results.items():
            if results["task_type"] == "classification":
                summary_data.append({
                    "Model": model_name,
                    "Task": "Classification",
                    "Accuracy": results["accuracy"],
                    "F1_Macro": results["f1_score"]["macro"],
                    "Precision_Macro": results["precision"]["macro"],
                    "Recall_Macro": results["recall"]["macro"],
                    "ROC_AUC": results.get("roc_auc", "N/A")
                })
            else:
                summary_data.append({
                    "Model": model_name,
                    "Task": "Regression",
                    "R2_Score": results["r2_score"],
                    "RMSE": results["rmse"],
                    "MAE": results["mae"],
                    "MAPE": results["mape"],
                    "ROC_AUC": "N/A"
                })
        
        return pd.DataFrame(summary_data)


# æµ‹è¯•ç”¨ä¾‹å’Œç¤ºä¾‹
def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹æ•°æ®"""
    np.random.seed(42)
    
    # åˆ†ç±»æ•°æ®
    n_samples = 1000
    X_class = np.random.randn(n_samples, 5)
    y_class = (X_class[:, 0] + X_class[:, 1] > 0).astype(int)
    y_prob_class = np.column_stack([1 - y_class, y_class])
    
    # å›å½’æ•°æ®
    X_reg = np.random.randn(n_samples, 3)
    y_reg = 2 * X_reg[:, 0] + 0.5 * X_reg[:, 1] - X_reg[:, 2] + np.random.randn(n_samples) * 0.1
    
    return X_class, y_class, y_prob_class, X_reg, y_reg


def test_classification_evaluation():
    """æµ‹è¯•åˆ†ç±»è¯„ä¼°åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•åˆ†ç±»æ¨¡å‹è¯„ä¼°")
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ModelEvaluator(task_type="classification")
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    X_class, y_class, y_prob_class, _, _ = create_sample_data()
    
    # æ¨¡æ‹Ÿä¸¤ä¸ªåˆ†ç±»æ¨¡å‹çš„é¢„æµ‹ç»“æœ
    y_pred_1 = (X_class[:, 0] + X_class[:, 1] > 0).astype(int)  # è¾ƒå¥½æ¨¡å‹
    y_pred_2 = np.random.choice([0, 1], size=len(y_class), p=[0.6, 0.4])  # è¾ƒå·®æ¨¡å‹
    
    # è¯„ä¼°æ¨¡å‹1
    results_1 = evaluator.evaluate_classification(
        y_class, y_pred_1, y_prob_class, 
        model_name="LogisticRegression"
    )
    
    # è¯„ä¼°æ¨¡å‹2
    results_2 = evaluator.evaluate_classification(
        y_class, y_pred_2, None,
        model_name="RandomClassifier"
    )
    
    # ç”ŸæˆæŠ¥å‘Š
    report_path = evaluator.generate_report("LogisticRegression", "html")
    
    # æ¯”è¾ƒæ¨¡å‹
    comparison_df = evaluator.compare_models(["LogisticRegression", "RandomClassifier"], "accuracy")
    
    # è·å–æœ€ä½³æ¨¡å‹
    best_model, best_score = evaluator.get_best_model("accuracy")
    
    return evaluator


def test_regression_evaluation():
    """æµ‹è¯•å›å½’è¯„ä¼°åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•å›å½’æ¨¡å‹è¯„ä¼°")
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ModelEvaluator(task_type="regression")
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    _, _, _, X_reg, y_reg = create_sample_data()
    
    # æ¨¡æ‹Ÿä¸¤ä¸ªå›å½’æ¨¡å‹çš„é¢„æµ‹ç»“æœ
    y_pred_1 = 2 * X_reg[:, 0] + 0.5 * X_reg[:, 1] - X_reg[:, 2] + np.random.randn(len(y_reg)) * 0.05  # è¾ƒå¥½æ¨¡å‹
    y_pred_2 = np.random.randn(len(y_reg)) * 2  # è¾ƒå·®æ¨¡å‹
    
    # è¯„ä¼°æ¨¡å‹1
    results_1 = evaluator.evaluate_regression(
        y_reg, y_pred_1,
        model_name="LinearRegression"
    )
    
    # è¯„ä¼°æ¨¡å‹2
    results_2 = evaluator.evaluate_regression(
        y_reg, y_pred_2,
        model_name="RandomRegressor"
    )
    
    # ç”ŸæˆæŠ¥å‘Š
    report_path = evaluator.generate_report("LinearRegression", "html")
    
    # æ¯”è¾ƒæ¨¡å‹
    comparison_df = evaluator.compare_models(["LinearRegression", "RandomRegressor"], "r2_score")
    
    # è·å–è¯„ä¼°æ‘˜è¦
    summary_df = evaluator.get_evaluation_summary()
    print("\nğŸ“Š è¯„ä¼°æ‘˜è¦:")
    print(summary_df.to_string(index=False))
    
    return evaluator


def comprehensive_test():
    """ç»¼åˆæµ‹è¯•"""
    print("ğŸš€ å¼€å§‹V3æ¨¡å‹è¯„ä¼°å™¨ç»¼åˆæµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•åˆ†ç±»è¯„ä¼°
    classifier_evaluator = test_classification_evaluation()
    
    # æµ‹è¯•å›å½’è¯„ä¼°
    regressor_evaluator = test_regression_evaluation()
    
    # ä¿å­˜ç»“æœ
    classifier_evaluator.save_results()
    regressor_evaluator.save_results()
    
    print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
    print("ğŸ“ æŸ¥çœ‹ç”Ÿæˆçš„è¯„ä¼°æŠ¥å‘Šå’Œå›¾è¡¨")
    
    return classifier_evaluator, regressor_evaluator


if __name__ == "__main__":
    # è¿è¡Œç»¼åˆæµ‹è¯•
    classifier_evaluator, regressor_evaluator = comprehensive_test()
    
    # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
    print("\n" + "=" * 60)
    print("ğŸ“š V3æ¨¡å‹è¯„ä¼°å™¨ä½¿ç”¨è¯´æ˜")
    print("=" * 60)
    print("""
ä¸»è¦åŠŸèƒ½:
1. ğŸ“Š å¤šæŒ‡æ ‡è¯„ä¼° - æ”¯æŒåˆ†ç±»å’Œå›å½’ä»»åŠ¡çš„å…¨é¢è¯„ä¼°
2. ğŸ“ˆ å¯è§†åŒ–åˆ†æ - æ··æ·†çŸ©é˜µã€ROCæ›²çº¿ã€æ®‹å·®åˆ†æç­‰
3. ğŸ”„ æ¨¡å‹æ¯”è¾ƒ - å¤šä¸ªæ¨¡å‹çš„æ€§èƒ½å¯¹æ¯”
4. ğŸ“„ æŠ¥å‘Šç”Ÿæˆ - HTMLå’ŒJSONæ ¼å¼çš„è¯¦ç»†æŠ¥å‘Š
5. ğŸ’¾ ç»“æœå­˜å‚¨ - è¯„ä¼°ç»“æœçš„ä¿å­˜å’ŒåŠ è½½
6. ğŸ’¡ æ™ºèƒ½è§£é‡Š - è‡ªåŠ¨ç”Ÿæˆç»“æœè§£é‡Šå’Œæ”¹è¿›å»ºè®®

ä½¿ç”¨ç¤ºä¾‹:
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ModelEvaluator(task_type="classification")
    
    # è¯„ä¼°åˆ†ç±»æ¨¡å‹
    results = evaluator.evaluate_classification(y_true, y_pred, y_prob, "MyModel")
    
    # ç”Ÿæˆå¯è§†åŒ–
    evaluator.plot_confusion_matrix("MyModel")
    
    # æ¯”è¾ƒæ¨¡å‹
    comparison = evaluator.compare_models(["Model1", "Model2"], "accuracy")
    
    # ç”ŸæˆæŠ¥å‘Š
    report_path = evaluator.generate_report("MyModel", "html")
    
    # ä¿å­˜ç»“æœ
    evaluator.save_results()
    """)